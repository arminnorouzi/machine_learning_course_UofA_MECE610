{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<a href=\"https://colab.research.google.com/github/arminnorouzi/machine_learning_course_UofA_MECE610/blob/main/L07_Generative_AI/L07b_Transformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ],
      "metadata": {
        "id": "6t-s30h0pJ3M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# L07b- Generative Adversarial Network (GAN)\n",
        "\n",
        "\n",
        "   - Developed by **Armin Norouzi**\n",
        "   - Compatible with Google Colaboratory\n",
        "\n",
        "\n",
        "**Table of Contents:**\n",
        "1.  Intorduction to Transformer\n",
        "2.  Tensorflow implementation of Transformer\n",
        "\n",
        "\n",
        "# 1. Intorduction to Transformer\n",
        "# 1.1. Generative Adversarial Network vs Transformer:\n",
        "\n",
        "In the previous section, we learn what GAN is and how it works. Now let's see what is transformers and why we need such a thing.\n",
        "\n",
        "We learned GAN is a type of neural network that consists of two networks, a **generator** and a **discriminator**. The generator tries to create new data samples that are similar to the input data, while the discriminator tries to distinguish between the real and fake data samples. The two networks are trained together in a way that the generator learns to create more realistic samples while the discriminator gets better at distinguishing between the real and fake samples.\n",
        "\n",
        "On the other hand, the **Transformer** is a type of neural network architecture that was introduced in the field of natural language processing (NLP). It is mainly used for tasks such as **language translation**, **text summarization**, and **language modelling**. The Transformer model consists of an **encoder** and a **decoder** that work together to process input sequences and generate output sequences. The encoder processes the input sequence and produces a hidden representation of the input. The decoder then takes the hidden representation and generates the output sequence. The Transformer uses a **self-attention mechanism** that allows the model to focus on different parts of the input sequence while processing it. Additionally, the Transformer uses a **positional encoding technique** to preserve the order of the input sequence, which is important for language tasks.\n",
        "\n",
        "While both models are used for different tasks, they do share some similarities. Both GAN and Transformer are deep learning models that are based on neural networks and use backpropagation to train their parameters. Additionally, they both have been used for generating realistic images and natural language text.\n",
        "\n",
        "However, the key difference between the two models is that GAN is used for generative tasks, while the Transformer is used for tasks related to natural language processing. **GANs generate new samples, while Transformers transform input sequences into output sequences.**\n",
        "\n",
        "# 1.2. We have RNN and LSTM; why do we need transformers?\n",
        "\n",
        "While RNNs and LSTMs are powerful models that have been used successfully in many natural language processing tasks, they have certain limitations that can make them less effective for certain tasks. Here are a few reasons why Transformers have emerged as an important alternative to RNNs and LSTMs:\n",
        "\n",
        "- **Long-term dependencies:** RNNs and LSTMs are designed to capture sequential dependencies in data, which makes them well-suited for modeling time-series data or sequences of variable length. However, they can struggle to capture long-term dependencies in data, particularly when the distance between the relevant elements in the sequence is large. Transformers are designed to explicitly model long-range dependencies using self-attention mechanisms, which allow them to attend to different parts of the input sequence and capture long-term relationships.\n",
        "\n",
        "- **Parallelization:** RNNs and LSTMs process data sequentially, which can make them slower and more computationally expensive than other models. Transformers, on the other hand, can process the entire input sequence in parallel, which makes them more efficient and faster to train. This is particularly important for large-scale natural language processing tasks that involve processing large amounts of data.\n",
        "\n",
        "- **Handling variable-length inputs:** RNNs and LSTMs are designed to handle input sequences of variable length, but they can struggle with very long sequences or sequences that contain significant amounts of noise or irrelevant information. Transformers are better suited for handling variable-length inputs and can effectively filter out noise or irrelevant information using their attention mechanisms.\n",
        "\n",
        "- **Attention-based mechanisms:** Transformers are designed to use attention-based mechanisms, which allow them to dynamically focus on different parts of the input sequence based on the context of the task. This makes them particularly well-suited for tasks that require the model to selectively attend to different parts of the input sequence, such as machine translation or question answering."
      ],
      "metadata": {
        "id": "TdIklliKrCXj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.2. Transformer components\n",
        "\n",
        "We briefly talked about the transformer component; let's dive into it in more detail. \n",
        "\n",
        "![picture](https://github.com/arminnorouzi/machine_learning_course_UofA_MECE610/blob/main/L07_Generative_AI/figures/transformers.png?raw=true) Source: [1-2]\n",
        "\n",
        "This figure shows transformers architecture schematically. The Transformer architecture consists of an encoder and decoder, which are composed of multiple layers that use attention and self-attention mechanisms to process the input and output sequences. The positional encoding technique is used to encode the position of tokens in the input sequence. These components work together to enable the Transformer to achieve state-of-the-art performance on various natural language processing tasks.\n",
        "\n",
        "- **Encoder:** The encoder is the part of the Transformer architecture that processes the input sequence and produces a hidden representation of the sequence. The input sequence is first transformed into a sequence of embeddings, which are then fed into a stack of identical layers. Each layer in the encoder stack consists of two sublayers: a self-attention layer and a feedforward layer. The self-attention layer allows the encoder to attend to different parts of the input sequence and capture long-range dependencies, while the feedforward layer applies a nonlinear transformation to the hidden representation.\n",
        "\n",
        "- **Decoder:** The decoder is the part of the Transformer architecture that generates the output sequence based on the hidden representation produced by the encoder. Like the encoder, the decoder also consists of a stack of identical layers, but each layer has three sublayers: a self-attention layer, an encoder-decoder attention layer, and a feedforward layer. The self-attention layer allows the decoder to attend to different parts of the output sequence, while the encoder-decoder attention layer allows the decoder to attend to different parts of the input sequence.\n",
        "\n",
        "- **Attention:** Attention is a mechanism in neural networks that allows the model to selectively attend to different parts of the input when making a prediction. In the Transformer architecture, attention is used in both the encoder and decoder. The attention mechanism calculates a weighted sum of the values of the input sequence, where the weights are determined by the similarity between the query and the keys. The attention mechanism allows the model to focus on different parts of the input sequence depending on the task at hand.\n",
        "\n",
        "- **Self-Attention Mechanism:** Self-attention is a specific type of attention mechanism that is used in the Transformer architecture. In self-attention, the input sequence is transformed into a sequence of query, key, and value vectors. The query vectors are used to calculate the attention weights for each position in the input sequence, based on the similarity between the query vector and the key vectors. The value vectors are then weighted by the attention weights and summed up to produce a weighted representation of the input sequence. This weighted representation is then used as the input for the next layer of the model. Self-attention allows the model to attend to different parts of the input sequence and capture long-range dependencies.\n",
        "\n",
        "- **Positional Encoding:** Positional encoding is a technique used in the Transformer architecture to encode the position of the tokens in the input sequence. Since the Transformer does not have a recurrence or convolutional structure that can capture the order of the input sequence, the positional encoding is added to each token's embedding to provide the model with information about the position of the token in the sequence. The positional encoding is calculated using a fixed function that takes into account the position of the token in the sequence and the dimension of the embedding. The result is then added to the token's embedding, allowing the model to differentiate between tokens that appear in different positions in the input sequence.\n",
        "\n",
        "\n",
        "For learning how these componenet working in a big picture, I refer you to [Google AI Blog post](https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html):\n",
        "\n",
        "> Neural networks for machine translation typically contain an encoder reading the input sentence and generating a representation of it. A decoder then generates the output sentence word by word while consulting the representation generated by the encoder. The Transformer starts by generating initial representations, or embeddings, for each word... Then, using self-attention, it aggregates information from all of the other words, generating a new representation per word informed by the entire context, represented by the filled balls. This step is then repeated multiple times in parallel for all words, successively generating new representations.\n",
        "\n",
        "<img src=\"https://www.tensorflow.org/images/tutorials/transformer/apply_the_transformer_to_machine_translation.gif\" alt=\"Applying the Transformer to machine translation\">\n",
        "\n",
        "Applying the Transformer to machine translation. Source: [Google AI Blog](https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html).\n"
      ],
      "metadata": {
        "id": "IxY_aNRasxEI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's dive into the code. I used most of these codes based on official tutorials from Tensorflow. I suggest going through that if you are more experienced in the machine learning world. I changed it slightly to explain the codes better so I can understand and teach! \n",
        "\n",
        "You can find tutorials here: [https://www.tensorflow.org/text/tutorials/transformer](https://www.tensorflow.org/text/tutorials/transformer)\n",
        "\n"
      ],
      "metadata": {
        "id": "CkaCms0ApeJi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Tensorflow implementation of Transformer\n",
        "## 2.1. Setting environement and Training data prepration\n",
        "\n",
        "First let's import the necessary libraries for building and training a transformer model"
      ],
      "metadata": {
        "id": "k3T-2EeiWo1E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install the most re version of TensorFlow to use the improved\n",
        "# masking support for `tf.keras.layers.MultiHeadAttention`.\n",
        "!apt install --allow-change-held-packages libcudnn8=8.1.0.77-1+cuda11.2\n",
        "!pip uninstall -y -q tensorflow keras tensorflow-estimator tensorflow-text\n",
        "!pip install protobuf~=3.20.3\n",
        "!pip install -q tensorflow_datasets\n",
        "!pip install -q -U tensorflow-text tensorflow"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W3usp6C16Gr7",
        "outputId": "dcfaf8a1-0d23-4d4d-b341-804a264b59ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following packages will be REMOVED:\n",
            "  libcudnn8-dev\n",
            "The following held packages will be changed:\n",
            "  libcudnn8\n",
            "The following packages will be DOWNGRADED:\n",
            "  libcudnn8\n",
            "0 upgraded, 0 newly installed, 1 downgraded, 1 to remove and 22 not upgraded.\n",
            "Need to get 430 MB of archives.\n",
            "After this operation, 1,153 MB disk space will be freed.\n",
            "Get:1 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64  libcudnn8 8.1.0.77-1+cuda11.2 [430 MB]\n",
            "Fetched 430 MB in 16s (26.7 MB/s)\n",
            "(Reading database ... 122518 files and directories currently installed.)\n",
            "Removing libcudnn8-dev (8.7.0.84-1+cuda11.8) ...\n",
            "update-alternatives: removing manually selected alternative - switching libcudnn to auto mode\n",
            "\u001b[1mdpkg:\u001b[0m \u001b[1;33mwarning:\u001b[0m downgrading libcudnn8 from 8.7.0.84-1+cuda11.8 to 8.1.0.77-1+cuda11.2\n",
            "(Reading database ... 122485 files and directories currently installed.)\n",
            "Preparing to unpack .../libcudnn8_8.1.0.77-1+cuda11.2_amd64.deb ...\n",
            "Unpacking libcudnn8 (8.1.0.77-1+cuda11.2) over (8.7.0.84-1+cuda11.8) ...\n",
            "Setting up libcudnn8 (8.1.0.77-1+cuda11.2) ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "import time\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import tensorflow_datasets as tfds\n",
        "import tensorflow as tf\n",
        "\n",
        "import tensorflow_text"
      ],
      "metadata": {
        "id": "QEqzBJen6GpK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's donwload  TED Talks dataset for [Portuguese-to-English translation using TensorFlow Datasets (TFDS)](https://www.tensorflow.org/datasets/catalog/ted_hrlr_translate#ted_hrlr_translatept_to_en): \n",
        "\n",
        "The `tfds.load()` function is used to load the dataset. The arguments passed to the function are:\n",
        "\n",
        "- `'ted_hrlr_translate/pt_to_en':` This specifies the name of the dataset to load, which is the TED Talks dataset for Portuguese-to-English translation.\n",
        "- `with_info=True:` This specifies that additional metadata about the dataset should be returned along with the dataset itself.\n",
        "- `as_supervised=True:` This specifies that the dataset should be returned as a tuple of (input, target) pairs, where input is a Portuguese sentence and target is the corresponding English translation."
      ],
      "metadata": {
        "id": "RidSkkSm6iu_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "examples, metadata = tfds.load('ted_hrlr_translate/pt_to_en',\n",
        "                               with_info=True,\n",
        "                               as_supervised=True)\n",
        "\n",
        "train_examples, val_examples = examples['train'], examples['validation']"
      ],
      "metadata": {
        "id": "e6q3zLPq6Gmk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's print out the first batch of examples from the Portuguese-to-English translation dataset loaded using TensorFlow Datasets (TFDS).\n",
        "\n",
        "The `train_examples.batch(3).take(1)` function call batches the dataset into groups of three examples and then takes the first batch. This means that the code will print out the first three examples in the dataset.\n",
        "\n",
        "The code then loops over the examples in the batch and prints out each example in both Portuguese and English. The `.decode('utf-8')` function call is used to convert the byte strings in the dataset to human-readable text."
      ],
      "metadata": {
        "id": "eP1InZU4Alhw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for pt_examples, en_examples in train_examples.batch(3).take(1):\n",
        "  print('> Examples in Portuguese:')\n",
        "  for pt in pt_examples.numpy():\n",
        "    print(pt.decode('utf-8'))\n",
        "  print()\n",
        "\n",
        "  print('> Examples in English:')\n",
        "  for en in en_examples.numpy():\n",
        "    print(en.decode('utf-8'))"
      ],
      "metadata": {
        "id": "V9V4giEb6GkS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.2. Set up the tokenizer\n",
        "\n",
        "Now time to tokenize our text. \n",
        "\n",
        "Let's download and load the tokenizers used for the Portuguese-to-English translation model provided by TensorFlow.\n",
        "\n",
        "This tutorial follows main tuterial from [tensorflow website](https://www.tensorflow.org/text/tutorials/transformer) and uses the tokenizers built in the [subword tokenizer](https://www.tensorflow.org/text/guide/subwords_tokenizer) tutorial. That tutorial optimizes two `text.BertTokenizer` objects (one for English, one for Portuguese) for **this dataset** and exports them in a TensorFlow `saved_model` format.\n",
        "\n",
        "The `tf.keras.utils.get_file()` function is used to download a zipped version of the tokenizers from the TensorFlow website. The first argument specifies the name of the downloaded file, while the second argument specifies the URL from which to download the file. The `cache_dir` argument specifies the directory in which to cache the downloaded file, while `cache_subdir` specifies the subdirectory in which to store the file. The extract argument specifies whether to extract the contents of the downloaded zip file.\n",
        "\n"
      ],
      "metadata": {
        "id": "-XDbzjvZBAbc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = 'ted_hrlr_translate_pt_en_converter'\n",
        "tf.keras.utils.get_file(\n",
        "    f'{model_name}.zip',\n",
        "    f'https://storage.googleapis.com/download.tensorflow.org/models/{model_name}.zip',\n",
        "    cache_dir='.', cache_subdir='', extract=True\n",
        ")"
      ],
      "metadata": {
        "id": "8NL4zNeC6Ghl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can use the `tf.saved_model.load()` function to load the tokenizers from the saved model. The `model_name` argument specifies the name of the saved model to load, which in this case is `ted_hrlr_translate_pt_en_converter`. "
      ],
      "metadata": {
        "id": "G7ROGNQfDJD-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizers = tf.saved_model.load(model_name)"
      ],
      "metadata": {
        "id": "OvNUC4xk6Gch"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The tokenize function is used to convert a group of strings into a batch of token IDs with padding. Prior to tokenization, the function splits punctuation, converts all letters to lowercase, and normalizes the input to Unicode format. However, since the input data has already been standardized, these steps are not apparent in the code. Let's check an exampple before and after tokenizer:"
      ],
      "metadata": {
        "id": "g-msMcDRELkN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('> This is a batch of strings:')\n",
        "for en in en_examples.numpy():\n",
        "  print(en.decode('utf-8'))"
      ],
      "metadata": {
        "id": "76-M1sSb6GaR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "encoded = tokenizers.en.tokenize(en_examples)\n",
        "\n",
        "print('> This is a padded-batch of token IDs:')\n",
        "for row in encoded.to_list():\n",
        "  print(row)"
      ],
      "metadata": {
        "id": "X3l2ELyt6GX8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `detokenize` method tries to transform the token IDs into text that can be easily read and understood by humans."
      ],
      "metadata": {
        "id": "Q1WOQPwmE4Ze"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "round_trip = tokenizers.en.detokenize(encoded)\n",
        "\n",
        "print('> This is human-readable text:')\n",
        "for line in round_trip.numpy():\n",
        "  print(line.decode('utf-8'))"
      ],
      "metadata": {
        "id": "zAcPDVsD6GVj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The lower level `lookup` method converts from token-IDs to token text:"
      ],
      "metadata": {
        "id": "f40UTTd4FY8h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('> This is the text split into tokens:')\n",
        "tokens = tokenizers.en.lookup(encoded)\n",
        "tokens"
      ],
      "metadata": {
        "id": "cRZfyI5x6GS7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's take a closer look at data by ploting the distribution of token lengths.\n",
        "\n",
        "First, an empty list called `lengths` is created to store the token lengths. Then, for each batch of 1024 examples in the training set, we can use the `tokenizers.pt.tokenize()` and `tokenizers.en.tokenize()` functions to tokenize the Portuguese and English examples, respectively. The `row_lengths()` function is then used to compute the number of tokens in each row of the tokenized data, and the resulting lengths are appended to the lengths list.\n",
        "\n",
        "After processing all of the batches, the `np.concatenate()` function is used to concatenate all of the token lengths into a single numpy array called all_lengths. This array is then used to create a histogram of token lengths using the `plt.hist()` function. "
      ],
      "metadata": {
        "id": "M2EcKB4FHOtk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lengths = []\n",
        "\n",
        "for pt_examples, en_examples in train_examples.batch(1024):\n",
        "  pt_tokens = tokenizers.pt.tokenize(pt_examples)\n",
        "  lengths.append(pt_tokens.row_lengths())\n",
        "  \n",
        "  en_tokens = tokenizers.en.tokenize(en_examples)\n",
        "  lengths.append(en_tokens.row_lengths())\n",
        "  print('.', end='', flush=True)\n",
        "\n",
        "all_lengths = np.concatenate(lengths)\n"
      ],
      "metadata": {
        "id": "MvX44eGv6GQl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.hist(all_lengths, np.linspace(0, 500, 101))\n",
        "plt.ylim(plt.ylim())\n",
        "avg_length = all_lengths.mean()\n",
        "plt.plot([avg_length, avg_length], plt.ylim())\n",
        "max_length = max(all_lengths)\n",
        "plt.plot([max_length, max_length], plt.ylim())\n",
        "plt.title(f'Maximum tokens per example: {max_length} and average tokens per example: {avg_length}');"
      ],
      "metadata": {
        "id": "8NnAXLqy6GON"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.3. Set up a data pipeline\n",
        "\n",
        "Let's write `prepare_batch()` function that prepares a batch of examples for training a machine translation model. The input to the function is a batch of Portuguese and English sentences, and the output is a tuple of two tensors representing the input and output sequences for the model.\n",
        "\n",
        "First, the Portuguese sentences are tokenized using the `tokenizers.pt.tokenize()` method, which returns a ragged tensor representing the tokenized sentences. The code then trims the tensor to a maximum length of`MAX_TOKENS using the pt[:, :MAX_TOKENS]` syntax, which selects the first `MAX_TOKENS` tokens from each sentence. The resulting tensor is converted to a dense tensor with zero padding using the `pt.to_tensor()` method.\n",
        "\n",
        "The English sentences are tokenized and trimmed in a similar way, but with an additional step. The `en[:, :(MAX_TOKENS+1)]` syntax selects the first `MAX_TOKENS+1` tokens from each sentence, which includes the start token [START] and end token [END]. The `en_inputs` tensor is created by selecting all but the last token from each sentence, which drops the end token. The `en_labels` tensor is created by selecting all but the first token from each sentence, which drops the start token.\n",
        "\n",
        "Finally, the function returns a tuple of two tensors, `pt, en_inputs` and `en_labels`, which represent the input and output sequences for the machine translation model. These tensors can be used to train the model using techniques such as teacher forcing, where the model is trained to predict the next token in the output sequence given the input sequence and the ground truth output sequence up to that point."
      ],
      "metadata": {
        "id": "JQXiIHPIU1Sx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_TOKENS=128\n",
        "def prepare_batch(pt, en):\n",
        "    \"\"\"\n",
        "    Preprocess a batch of Portuguese and English sentences for training a machine translation model.\n",
        "\n",
        "    Args:\n",
        "        pt: A tensor of Portuguese sentences of shape (batch_size,) and dtype tf.string.\n",
        "        en: A tensor of English sentences of shape (batch_size,) and dtype tf.string.\n",
        "\n",
        "    Returns:\n",
        "        A tuple of two tensors representing the input and output sequences for the model, and a tensor of shape\n",
        "        (batch_size, max_length) representing the ground truth output sequences. The input sequence tensor has shape\n",
        "        (batch_size, max_length) and dtype tf.int64, and the output sequence tensor has shape (batch_size, max_length)\n",
        "        and dtype tf.int64.\n",
        "    \"\"\"\n",
        "    pt = tokenizers.pt.tokenize(pt)      # Output is ragged.\n",
        "    pt = pt[:, :MAX_TOKENS]    # Trim to MAX_TOKENS.\n",
        "    pt = pt.to_tensor()  # Convert to 0-padded dense Tensor\n",
        "\n",
        "    en = tokenizers.en.tokenize(en)\n",
        "    en = en[:, :(MAX_TOKENS+1)]\n",
        "    en_inputs = en[:, :-1].to_tensor()  # Drop the [END] tokens\n",
        "    en_labels = en[:, 1:].to_tensor()   # Drop the [START] tokens\n",
        "\n",
        "    return (pt, en_inputs), en_labels"
      ],
      "metadata": {
        "id": "C4pLQMs86GLz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's take dataset and converted it into batches that are ready to be fed to the model. \n",
        "\n",
        "The following function shuffles the examples in the dataset and batches them into batches of size `BATCH_SIZE`. It then applies the prepare_batch function to each batch, which tokenizes the text and prepares the input and output sequences for the model. Finally, it prefetches the batches to improve performance during training. The `BUFFER_SIZE` parameter determines the number of examples to load into memory for shuffling. The `tf.data.AUTOTUNE` argument allows TensorFlow to automatically tune the input pipeline for optimal performance."
      ],
      "metadata": {
        "id": "bPJefg1aV5k_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BUFFER_SIZE = 20000\n",
        "BATCH_SIZE = 64"
      ],
      "metadata": {
        "id": "hTe_qJSM6GJn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_batches(ds):\n",
        "  \"\"\"\n",
        "  This function takes a TensorFlow dataset 'ds' and processes it into batches that are ready to be fed to the model. \n",
        "\n",
        "  Parameters:\n",
        "  ds (tf.data.Dataset): TensorFlow dataset to be processed into batches\n",
        "\n",
        "  Returns:\n",
        "  tf.data.Dataset: Processed and batched TensorFlow dataset\n",
        "\n",
        "  \"\"\"\n",
        "  return (\n",
        "      ds\n",
        "      .shuffle(BUFFER_SIZE)\n",
        "      .batch(BATCH_SIZE)\n",
        "      .map(prepare_batch, tf.data.AUTOTUNE)\n",
        "      .prefetch(buffer_size=tf.data.AUTOTUNE))"
      ],
      "metadata": {
        "id": "rmDdwILj6GG7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's see if we did everything write by testing dataset"
      ],
      "metadata": {
        "id": "DuFCoHaqXJFm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create training and validation set batches.\n",
        "train_batches = make_batches(train_examples)\n",
        "val_batches = make_batches(val_examples)\n",
        "\n",
        "for (pt, en), en_labels in train_batches.take(1):\n",
        "  break\n",
        "\n",
        "print(f'pt.shape: {pt.shape}')\n",
        "print(f'en_labels.shape: {en_labels.shape}')"
      ],
      "metadata": {
        "id": "bLLjae4_6GEv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The function `make_batches` prepares `tf.data.Dataset` objects for training a Keras model. The model is expected to take input in the form of pairs of tokenized Portuguese and English sequences `(pt, en)`, and predict the English sequences shifted by one token. This is known as \"teacher forcing\" because at each timestep, the model receives the true value as input for the next timestep regardless of its previous output. This is a simple and efficient way to train a text generation model as the outputs can be computed in parallel.\n",
        "\n",
        "While one might expect the `input, output` pairs to simply be the `Portuguese, English` sequences, this setup adds \"context\" to the model by conditioning it on the Portuguese sequence. It is possible to train a model without conditioning it on the Portuguese sequence, but that would require writing an inference loop and passing the model's output back to the input. This is slower and harder to learn but can result in a more stable model as the model has to learn to correct its own errors during training.\n",
        "\n",
        "The `en` and `en_labels` are the same, just shifted by 1:"
      ],
      "metadata": {
        "id": "yTNer5cbYdQr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'en[0][:10]: {en[0][:10]}')\n",
        "print(f'en_labels[0][:10]: {en_labels[0][:10]}')"
      ],
      "metadata": {
        "id": "RnMFUfsw6GCP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.4. Define the components"
      ],
      "metadata": {
        "id": "IQnXZvOBY1DQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.4.1. The embedding and positional encoding layer\n",
        "\n",
        "Both the encoder and decoder components use the same logic to convert input tokens to vectors. This is done using a `tf.keras.layers.Embedding` layer, which creates a vector representation for each token in the input sequence.\n",
        "\n",
        "The attention layers in the model don't rely on the order of the tokens in the input sequence, because the model doesn't contain any recurrent or convolutional layers that would inherently capture the sequence order. Without a way to identify the word order, the model would see the input sequence as a \"bag of words\", where the order of the tokens doesn't matter. For example, the sequences \"how are you\", \"how you are\", and \"you how are\" would all be seen as identical by the model.\n",
        "\n",
        "To overcome this issue, a Transformer model adds a \"Positional Encoding\" to the embedding vectors. The Positional Encoding uses a set of sines and cosines at different frequencies across the sequence. Each token in the input sequence has a unique positional encoding that captures its position in the sequence. The nearby tokens in the sequence will have similar positional encodings. By incorporating this information into the input representation, the model can maintain the sequential order of the input tokens and better understand the meaning of the sentence.\n",
        "\n",
        "The formula for calculating the positional encoding is as follows:\n",
        "\n",
        "$$\\Large{PE_{(pos, 2i)} = \\sin(pos / 10000^{2i / d_{model}})} $$\n",
        "$$\\Large{PE_{(pos, 2i+1)} = \\cos(pos / 10000^{2i / d_{model}})} $$\n",
        "\n",
        "Now, let's implement it:\n",
        "\n",
        "The `positional_encoding` function generates a matrix of position encodings for the input sequence. The purpose of positional encoding is to add information about the position of each token in the sequence, so that the self-attention mechanism in the transformer can distinguish between the different positions of the tokens.\n",
        "\n",
        "The function takes two arguments: `length`, which specifies the length of the input sequence, and `depth`, which specifies the dimensionality of the encoding.\n",
        "\n",
        "The function first creates two matrices: `positions` and `depths`. positions has shape `(length, 1)` and contains the indices of the positions in the input sequence. `depths` has shape `(1, depth/2)` and contains values ranging from `0` to `(depth/2)-1`, which are then normalized by `depth/2`.\n",
        "\n",
        "The function then calculates the angle rates using the formula `1 / (10000**depths)`, which has shape `(1, depth/2)`. The angle rates are used to calculate the angle radians using the formula `positions * angle_rates`, which has shape `(length, depth/2)`.\n",
        "\n",
        "Finally, the function concatenates the sine and cosine values of the angle radians along the last axis to create the position encoding matrix, which has shape `(length, depth)`. The resulting matrix is then cast to `tf.float32` and returned."
      ],
      "metadata": {
        "id": "otp1lQsDIW4Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def positional_encoding(length, depth):\n",
        "  \"\"\"\n",
        "  Generates a matrix of position encodings for an input sequence.\n",
        "\n",
        "  Args:\n",
        "      length: An integer representing the length of the input sequence.\n",
        "      depth: An integer representing the dimensionality of the encoding.\n",
        "\n",
        "  Returns:\n",
        "      A `tf.Tensor` of shape `(length, depth)` representing the position encoding matrix.\n",
        "  \"\"\"\n",
        "  depth = depth/2\n",
        "\n",
        "  positions = np.arange(length)[:, np.newaxis]     # (seq, 1)\n",
        "  depths = np.arange(depth)[np.newaxis, :]/depth   # (1, depth)\n",
        "  \n",
        "  angle_rates = 1 / (10000**depths)         # (1, depth)\n",
        "  angle_rads = positions * angle_rates      # (pos, depth)\n",
        "\n",
        "  pos_encoding = np.concatenate(\n",
        "      [np.sin(angle_rads), np.cos(angle_rads)],\n",
        "      axis=-1) \n",
        "\n",
        "  return tf.cast(pos_encoding, dtype=tf.float32)"
      ],
      "metadata": {
        "id": "anu5uaGw6F_Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The position encoding function uses a series of sines and cosines that oscillate at various frequencies based on where they are positioned along the depth of the embedding vector. These oscillations occur across the position axis. Let's visualize it here:"
      ],
      "metadata": {
        "id": "4dPq0tKLOjLM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title\n",
        "pos_encoding = positional_encoding(length=2048, depth=512)\n",
        "\n",
        "# Check the shape.\n",
        "print(pos_encoding.shape)\n",
        "\n",
        "# Plot the dimensions.\n",
        "plt.pcolormesh(pos_encoding.numpy().T, cmap='RdBu')\n",
        "plt.ylabel('Depth')\n",
        "plt.xlabel('Position')\n",
        "plt.colorbar()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "m7sZHG0I6F88"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The purpose of this plot is to visualize the positional encoding matrix and see how it changes across different positions and depths in the sequence. It also helps to ensure that the encoding values are properly normalized and distributed across the matrix\n",
        "\n",
        "Let's visualize the cosine similarity between the positional encoding vector at index 1000 and all other vectors in the positional encoding matrix.\n",
        "\n",
        "The positional encoding vectors are first normalized using `L2` normalization. The code then calculates the dot product between the positional encoding vector at index 1000 and all other vectors in the matrix using the einsum function. The resulting dot products are plotted in a graph with the y-axis representing the cosine similarity values between the vectors.\n"
      ],
      "metadata": {
        "id": "QkWh13SOPAec"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title\n",
        "pos_encoding/=tf.norm(pos_encoding, axis=1, keepdims=True)\n",
        "p = pos_encoding[1000]\n",
        "dots = tf.einsum('pd,d -> p', pos_encoding, p)\n",
        "plt.subplot(2,1,1)\n",
        "plt.plot(dots)\n",
        "plt.ylim([0,1])\n",
        "plt.plot([950, 950, float('nan'), 1050, 1050],\n",
        "         [0,1,float('nan'),0,1], color='k', label='Zoom')\n",
        "plt.legend()\n",
        "plt.subplot(2,1,2)\n",
        "plt.plot(dots)\n",
        "plt.xlim([950, 1050])\n",
        "plt.ylim([0,1])\n"
      ],
      "metadata": {
        "id": "4yGGwm896F6c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The first plot shows the entire cosine similarity graph, while the second plot zooms in on the cosine similarity values between index 950 and 1050.\n",
        "\n",
        "This visualization helps to illustrate how the positional encoding vectors encode the position information of each token in the sequence. The cosine similarity values are highest for vectors that are close to each other along the position axis, indicating that they have similar positional information.\n",
        "\n",
        "Now let's put things togher and create `PositionEmbedding` class. This is a `tf.keras.layers.Layer` class that combines an embedding layer and a positional encoding layer to create a layer that can be used to encode input sequences in a transformer model.\n",
        "\n",
        "The class takes two arguments: `vocab_size` which is the size of the vocabulary of the input sequences and `d_model` which is the size of the embedding and positional encoding vectors.\n",
        "\n",
        "In the constructor, it creates an `Embedding` layer that maps input tokens to their corresponding embedding vectors, and a positional encoding matrix of shape `(max_length, d_model)` using the `positional_encoding` function.\n",
        "\n",
        "The `compute_mask` method of this class returns a mask with the same shape as the input tensor to the embedding layer.\n",
        "\n",
        "In the call method, the input tensor is first passed through the embedding layer, and then scaled by the square root of the `d_model` value. Then, the positional encoding matrix is added to the embedding output corresponding to each input token. Finally, the encoded input sequence is returned."
      ],
      "metadata": {
        "id": "49bRm5m8POPl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEmbedding(tf.keras.layers.Layer):\n",
        "  \"\"\"\n",
        "  This layer combines the input embedding with a positional encoding that helps the Transformer to understand\n",
        "  the relative position of the tokens in a sequence. It takes an input sequence of tokens and converts it to \n",
        "  a sequence of embedding vectors, then adds positional information to it.\n",
        "\n",
        "  Attributes:\n",
        "      vocab_size (int): The size of the vocabulary, i.e., the number of unique tokens in the input sequence.\n",
        "      d_model (int): The number of dimensions in the embedding vector.\n",
        "\n",
        "  Methods:\n",
        "      compute_mask(*args, **kwargs): This method computes the mask to be applied to the embeddings.\n",
        "      call(x): This method performs the computation for the layer.\n",
        "\n",
        "  \"\"\"\n",
        "  def __init__(self, vocab_size, d_model):\n",
        "    \"\"\"\n",
        "    Initializes the PositionalEmbedding layer.\n",
        "\n",
        "    Args:\n",
        "        vocab_size (int): The size of the vocabulary, i.e., the number of unique tokens in the input sequence.\n",
        "        d_model (int): The number of dimensions in the embedding vector.\n",
        "    \"\"\"\n",
        "    super().__init__()\n",
        "    self.d_model = d_model\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, d_model, mask_zero=True) \n",
        "    self.pos_encoding = positional_encoding(length=2048, depth=d_model)\n",
        "\n",
        "  def compute_mask(self, *args, **kwargs):\n",
        "    \"\"\"\n",
        "    Computes the mask to be applied to the embeddings.\n",
        "\n",
        "    Args:\n",
        "        *args: Variable length argument list.\n",
        "        **kwargs: Arbitrary keyword arguments.\n",
        "\n",
        "    Returns:\n",
        "        Mask to be applied to the embeddings.\n",
        "    \"\"\"\n",
        "    return self.embedding.compute_mask(*args, **kwargs)\n",
        "\n",
        "  def call(self, x):\n",
        "    \"\"\"\n",
        "    Computes the output of the layer.\n",
        "\n",
        "    Args:\n",
        "        x (tf.Tensor): Input sequence of tokens.\n",
        "\n",
        "    Returns:\n",
        "        The output sequence of embedding vectors with added positional information.\n",
        "    \"\"\"\n",
        "    length = tf.shape(x)[1]\n",
        "    x = self.embedding(x)\n",
        "    # This factor sets the relative scale of the embedding and positonal_encoding.\n",
        "    x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "    x = x + self.pos_encoding[tf.newaxis, :length, :]\n",
        "    return x\n"
      ],
      "metadata": {
        "id": "hNB6rkPK6F4J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Note: According to [tensflow tutorial](https://www.tensorflow.org/text/tutorials/transformer), the [original paper](https://arxiv.org/pdf/1706.03762.pdf), section 3.4 and 5.1, uses a single tokenizer and weight matrix for both the source and target languages. This tutorial uses two separate tokenizers and weight matrices.\n",
        "\n",
        "\n",
        "Let's create two instances of the `PositionalEmbedding` class, one for the **Portuguese** tokenizer and one for the **English** tokenizer. We pass the vocabulary size of each tokenizer and a value for `d_model` which is the dimensionality of the embedding vector.\n",
        "\n",
        "Then we call these instances on our tokenized Portuguese and English sentences (`pt` and `en`), respectively. The output of each call is an embedded representation of the sentence, where each token is represented as a vector with a positional encoding added to it, as described in the `PositionalEmbedding` class.\n",
        "\n",
        "The resulting embeddings can be used as input to the encoder and decoder of a Transformer model."
      ],
      "metadata": {
        "id": "iMsr1w65VfSp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embed_pt = PositionalEmbedding(vocab_size=tokenizers.pt.get_vocab_size(), d_model=512)\n",
        "embed_en = PositionalEmbedding(vocab_size=tokenizers.en.get_vocab_size(), d_model=512)\n",
        "\n",
        "pt_emb = embed_pt(pt)\n",
        "en_emb = embed_en(en)"
      ],
      "metadata": {
        "id": "pE3inedG6F1z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In Keras, masking is used to indicate timesteps that should be ignored during processing, for example, padding timesteps. The `_keras_mask` attribute returns a boolean tensor with the same shape as en_emb that indicates which timesteps should be masked (True for masked timesteps, False for unmasked timesteps). If a timestep is masked, it means that its corresponding values will be ignored during computation.\""
      ],
      "metadata": {
        "id": "BxqDfGhNZVn8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "en_emb._keras_mask"
      ],
      "metadata": {
        "id": "IhyHR2At6Fzk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.4.5. Add and normalize"
      ],
      "metadata": {
        "id": "VGZwRLDwZeRI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The \"Add & Norm\" blocks are used in the Transformer model and help with efficient training. These blocks consist of a residual connection, which provides a direct path for the gradient and ensures that vectors are updated instead of replaced by the attention layers, and a `LayerNormalization` layer that maintains a reasonable scale for the outputs. These blocks are scattered throughout the model, and the code is organized around them. Custom layer classes are defined for each block. The Add layer is used in the implementation to ensure that Keras masks are propagated since the + operator does not do that.\n",
        "\n",
        "> Note: In the case of residual addition, the original input to a layer is added to the output of that layer, creating a \"residual\" connection that allows the gradient to bypass the layer during backpropagation. This helps to prevent the gradients from vanishing and allows the weights to continue updating during training. In the case of the Transformer model, residual connections are used in combination with layer normalization to help with training efficiency and maintain a reasonable scale for the outputs."
      ],
      "metadata": {
        "id": "y16GVDIjbrWF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.4.6. Attention layer\n",
        "\n",
        "The model includes Attention blocks, each containing a layers.MultiHeadAttention, a `layers.LayerNormalization`, and a `layers.Add`. To create these attention layers, we first define a base class that includes these three components, and then we create specific subclasses for each use case. Although it requires writing more code, this approach helps keep the implementation organized and easy to understand.\n",
        "\n",
        "The class contains three layers, `tf.keras.layers.MultiHeadAttention`, `tf.keras.layers.LayerNormalization`, and `tf.keras.layers.Add`. \n",
        "\n",
        "- The `MultiHeadAttention` layer is responsible for computing the attention weights between the input and output sequences. \n",
        "- The `LayerNormalization` layer normalizes the activations of the layer across the batch and feature dimensions. \n",
        "- The `Add` layer adds the output of the `MultiHeadAttention` layer to the original input sequence using a residual connection.\n",
        "\n",
        "By creating a base class with these layers, we can reuse this code to create different attention mechanisms by inheriting from this class and defining the specific implementation details. This helps keep the code organized and clear."
      ],
      "metadata": {
        "id": "SnmrtMYMfNw_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BaseAttention(tf.keras.layers.Layer):\n",
        "  \"\"\"\n",
        "  Base Attention layer class that contains a MultiHeadAttention, LayerNormalization and Add layer.\n",
        "\n",
        "  Attributes:\n",
        "  -----------\n",
        "  kwargs: dict\n",
        "      keyword arguments that will be passed to the MultiHeadAttention layer during initialization.\n",
        "\n",
        "  Methods:\n",
        "  --------\n",
        "  call(inputs, mask=None, training=None):\n",
        "      Performs a forward pass on the input and returns the output.\n",
        "\n",
        "  \"\"\"\n",
        "  def __init__(self, **kwargs):\n",
        "    \"\"\"\n",
        "    Initializes a new instance of the BaseAttention layer class.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    kwargs: dict\n",
        "        keyword arguments that will be passed to the MultiHeadAttention layer during initialization.\n",
        "    \"\"\"\n",
        "    super().__init__()\n",
        "    self.mha = tf.keras.layers.MultiHeadAttention(**kwargs)\n",
        "    self.layernorm = tf.keras.layers.LayerNormalization()\n",
        "    self.add = tf.keras.layers.Add()"
      ],
      "metadata": {
        "id": "87yzDMJyZeEX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**How attention works?**\n",
        "\n",
        "In an attention layer, there are two inputs the **query sequence** and the **context sequence**. The query sequence is the sequence being processed, while the context sequence is the sequence being attended to. The output has the same shape as the query sequence.\n",
        "\n",
        "The operation of an attention layer is often compared to that of a dictionary lookup, but with fuzzy, differentiable, and vectorized characteristics. Just like a dictionary lookup, a query is used to search for relevant information, which is represented as **keys** and **values**. When searching for a query in a regular dictionary, the matching key and its corresponding value are returned. However, in a fuzzy dictionary, a query does not need to match perfectly with a key for the value to be returned. \n",
        "\n",
        "For example, if we searched for the key \"species\" in the dictionary `{'color': 'blue', 'age': 22, 'type': 'pickup'}`, it might return the value \"pickup\" as the best match for the query.\n",
        "\n",
        "**An attention layer works similarly to a fuzzy dictionary lookup, but instead of returning a single value, it combines multiple values based on how well they match with the query.** The query, key, and value in an attention layer are each represented as vectors. Instead of using hash lookup, the attention layer combines the query and key vectors to determine how well they match, which is known as the attention score. The values are then combined by taking the weighted average of all values, where the weights are determined by the attention scores.\n",
        "\n",
        "In the context of NLP, the query sequence can provide a query vector at each location, while the context sequence serves as the dictionary, with a key and value vector at each location. Before using the input vectors, the `layers.MultiHeadAttention` layer includes `layers.Dense` layers to project the input vectors."
      ],
      "metadata": {
        "id": "-rzqi3Fyk821"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "So now let's use this class to create other attention layers. We will create:\n",
        "\n",
        "- The cross attention layer: Decoder-encoder attention\n",
        "- The global self attention layer: Encoder self-attention\n",
        "- The causal self attention layer: Decoder self-attention\n",
        "\n",
        "\n",
        "![picture](https://github.com/arminnorouzi/machine_learning_course_UofA_MECE610/blob/main/L07_Generative_AI/figures/transformers.png?raw=true) Source: [1-2]"
      ],
      "metadata": {
        "id": "H502TypxjzPS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2.4.6.1. The cross attention layer: Decoder-encoder attention\n"
      ],
      "metadata": {
        "id": "sGmU9cLKnHeM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's write `CrossAttention` class by inheriting it from the `BaseAttention` class, which contains a multi-head attention layer, a layer normalization layer, and an add layer.\n",
        "\n",
        "The `call` method takes two input arguments, `x` and `context`. `x` is the query sequence, which is being processed and doing the attending, and `context` is the `context` sequence, which is being attended to.\n",
        "\n",
        "The `call` method passes `x` and `context` to the `self.mha` (multi-head attention) layer, which returns an attention output tensor and attention scores tensor. The `self.last_attn_scores` attribute is set to the attention scores tensor for plotting later.\n",
        "\n",
        "Next, the attention output tensor is added to the original `x` tensor using the `self.add` layer, and the result is normalized using the `self.layernorm` layer. The final output is then returned."
      ],
      "metadata": {
        "id": "Nv31vak6nYNR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CrossAttention(BaseAttention):\n",
        "  \"\"\"\n",
        "  A class that implements cross-attention mechanism by inheriting from BaseAttention class.\n",
        "  Cross-attention is used to process two different sequences and attends to the context sequence while processing the query sequence.\n",
        "  Inherits:\n",
        "      BaseAttention: A base class that defines the MultiHeadAttention layer, LayerNormalization, and Add operation.\n",
        "  Args:\n",
        "      **kwargs: Arguments to pass to the MultiHeadAttention layer.\n",
        "  \"\"\"\n",
        "  def call(self, x, context):\n",
        "    \"\"\"\n",
        "    The call function that performs the cross-attention operation.\n",
        "\n",
        "    Args:\n",
        "        x: The query sequence tensor, shape=(batch_size, seq_len, embedding_dim)\n",
        "        context: The context sequence tensor, shape=(batch_size, seq_len, embedding_dim)\n",
        "\n",
        "    Returns:\n",
        "        The attended output tensor, shape=(batch_size, seq_len, embedding_dim)\n",
        "    \"\"\"\n",
        "    attn_output, attn_scores = self.mha(\n",
        "        query=x,\n",
        "        key=context,\n",
        "        value=context,\n",
        "        return_attention_scores=True)\n",
        "   \n",
        "    # Cache the attention scores for plotting later.\n",
        "    self.last_attn_scores = attn_scores\n",
        "\n",
        "    x = self.add([x, attn_output])\n",
        "    x = self.layernorm(x)\n",
        "\n",
        "    return x"
      ],
      "metadata": {
        "id": "sMK1HCrdZeCM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_ca = CrossAttention(num_heads=2, key_dim=512)\n",
        "\n",
        "print(pt_emb.shape)\n",
        "print(en_emb.shape)\n",
        "print(sample_ca(en_emb, pt_emb).shape)"
      ],
      "metadata": {
        "id": "3ycWWzWlZd_x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The output `length` is the length of the query sequence, and not the length of the context `key/value` sequence."
      ],
      "metadata": {
        "id": "Xtl7SCLwsmcN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2.4.6.2. The global self attention layer: Encoder self-attention\n",
        "This layer is responsible for processing the context sequence, and propagating information along its length. Now let's write `GlobalSelfAttention` by inheriting from `baseAttention` layer. \n",
        "\n",
        "In `GlobalSelfAttention`, there is only one input `x`, which is a sequence of vectors that represents the sequence being processed. This input is used as the query, key and value input for the multi-head attention (MHA) mechanism. The MHA computes a weighted average of the values based on how well the query matches the keys, where the attention scores determine the weight of each value.\n",
        "\n",
        "In other words, the MHA learns to selectively focus on different parts of the input sequence, which can help the model capture relevant information for a particular task. In `GlobalSelfAttention`, since the input sequence is used for both `query` and `key`, it captures the relationship between each position and all the other positions in the sequence.\n",
        "\n",
        "Finally, the output of the MHA is added back to the original input, followed by layer normalization, to obtain the final output of the attention layer. The normalization helps to stabilize the training process and improves the performance of the model."
      ],
      "metadata": {
        "id": "uDPuswcCnUGL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GlobalSelfAttention(BaseAttention):\n",
        "  def call(self, x):\n",
        "    \"\"\"\n",
        "    Apply the global self-attention mechanism to the input sequence.\n",
        "\n",
        "    Args:\n",
        "        x: A tensor of shape `(batch_size, seq_len, embedding_dim)` \n",
        "        representing the input sequence.\n",
        "\n",
        "    Returns:\n",
        "        A tensor of the same shape as the input, representing the sequence \n",
        "        after being transformed by the self-attention mechanism.\n",
        "    \"\"\"\n",
        "    attn_output = self.mha(\n",
        "        query=x,\n",
        "        value=x,\n",
        "        key=x)\n",
        "    x = self.add([x, attn_output])\n",
        "    x = self.layernorm(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "joxGnjLeZd67"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_gsa = GlobalSelfAttention(num_heads=2, key_dim=512)\n",
        "\n",
        "print(pt_emb.shape)\n",
        "print(sample_gsa(pt_emb).shape)"
      ],
      "metadata": {
        "id": "0amzzHxgZd4n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Output tensor has the same shape as the input"
      ],
      "metadata": {
        "id": "WBM8rwQFthQG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2.4.6.3. The causal self attention layer: Decoder self-attention"
      ],
      "metadata": {
        "id": "X_BU6LJBnVXa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This layer does a similar job as the global self attention layer, for the output sequence.  Now let's write `CausalSelfAttention` by inheriting from `baseAttention` layer.\n",
        "\n",
        "The `CausalSelfAttention` class is a type of self-attention layer used in neural networks for sequence modeling tasks where the output at each time step can only depend on previous time steps, and not on future time steps. In such tasks, the causal self-attention layer is used to enforce the constraint that the model can only attend to the previous time steps during the decoding process.\n",
        "\n",
        "The `call` method of this class takes a tensor `x` as input, and applies the causal self-attention mechanism to it. Specifically, the method uses the mha method (multi-head attention) of the `BaseAttention` class with the `query`, `key`, and `value` inputs set to `x`. Additionally, the `use_causal_mask` argument of the mha method is set to `True`, which applies a causal mask to the attention scores to **ensure that the model can only attend to previous time steps**.\n",
        "\n",
        "After applying the causal self-attention mechanism, the method adds the output to the original input tensor `x`, and normalizes the result using layer normalization. Finally, the normalized tensor is returned as the output of the method."
      ],
      "metadata": {
        "id": "uZdM3DCAt2lT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CausalSelfAttention(BaseAttention):\n",
        "  \"\"\"\n",
        "  Call self attention on the input sequence, ensuring that each position in the \n",
        "  output depends only on previous positions (i.e. a causal model).\n",
        "\n",
        "  Args:\n",
        "      x: Input sequence tensor of shape `(batch_size, seq_len, embed_dim)`.\n",
        "\n",
        "  Returns:\n",
        "      Output sequence tensor of the same shape as the input, after self-attention \n",
        "      and residual connection with layer normalization applied.\n",
        "  \"\"\"\n",
        "  def call(self, x):\n",
        "    attn_output = self.mha(\n",
        "        query=x,\n",
        "        value=x,\n",
        "        key=x,\n",
        "        use_causal_mask = True)\n",
        "    x = self.add([x, attn_output])\n",
        "    x = self.layernorm(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "L5LcuQAhZd2T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_csa = CausalSelfAttention(num_heads=2, key_dim=512)\n",
        "\n",
        "print(en_emb.shape)\n",
        "print(sample_csa(en_emb).shape)"
      ],
      "metadata": {
        "id": "8mQMwgDNZdzz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The output for early sequence elements doesn't depend on later elements, so it shouldn't matter if you trim elements before or after applying the layer:"
      ],
      "metadata": {
        "id": "WyWbVmXAwF0W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "out1 = sample_csa(embed_en(en[:, :3])) \n",
        "out2 = sample_csa(embed_en(en))[:, :3]\n",
        "\n",
        "tf.reduce_max(abs(out1 - out2)).numpy()"
      ],
      "metadata": {
        "id": "eMyighGzZdxP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "basiclly the difference between before and after triming `tf.reduce_max(abs(out1 - out2)).numpy()` is zero!\n"
      ],
      "metadata": {
        "id": "Xz5-zCFmx3Gr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.4.7. The feed forward network\n",
        "\n",
        "Now let's implement feedforward network. \n",
        "\n",
        "The `FeedForward` class is a custom layer in TensorFlow that implements a feedforward neural network. It is commonly used in transformer-based models like `BERT` and `GPT-2` to process each token's representation.\n",
        "\n",
        "The layer takes as input a tensor `x` with shape `(batch_size, seq_len, d_model)`, where `d_model` is the size of the last dimension. It passes the tensor `x` through a feedforward network consisting of two dense layers with dff hidden units and a `relu` activation function. A `dropout_rate` is also applied after the first dense layer to prevent overfitting. The output of the feedforward network is added to the original input `x` via the Add() layer. Finally, the output is normalized using the `LayerNormalization()` layer.\n",
        "\n",
        "The `FeedForward` layer can learn a more complex function than a simple linear layer, which makes it useful for modeling non-linear relationships between the input and output."
      ],
      "metadata": {
        "id": "oofiI_quyCz6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForward(tf.keras.layers.Layer):\n",
        "  \"\"\"\n",
        "  Implements the feedforward sublayer of the transformer block.\n",
        "\n",
        "  Parameters:\n",
        "  -----------\n",
        "  d_model: int\n",
        "      The number of expected features in the input and output.\n",
        "  dff: int\n",
        "      The number of neurons in the first Dense layer.\n",
        "  dropout_rate: float, optional (default=0.1)\n",
        "      The dropout rate to use.\n",
        "\n",
        "  Attributes:\n",
        "  -----------\n",
        "  seq: tf.keras.Sequential\n",
        "      The sequential model that applies the two Dense layers and Dropout.\n",
        "  add: tf.keras.layers.Add\n",
        "      The addition layer that adds the residual connection.\n",
        "  layer_norm: tf.keras.layers.LayerNormalization\n",
        "      The normalization layer applied to the output.\n",
        "\n",
        "  Methods:\n",
        "  --------\n",
        "  call(x):\n",
        "      Computes the feedforward sublayer on the input tensor x and returns the output.\n",
        "\n",
        "  \"\"\"\n",
        "  def __init__(self, d_model, dff, dropout_rate=0.1):\n",
        "    super().__init__()\n",
        "    self.seq = tf.keras.Sequential([\n",
        "      tf.keras.layers.Dense(dff, activation='relu'),\n",
        "      tf.keras.layers.Dense(d_model),\n",
        "      tf.keras.layers.Dropout(dropout_rate)\n",
        "    ])\n",
        "    self.add = tf.keras.layers.Add()\n",
        "    self.layer_norm = tf.keras.layers.LayerNormalization()\n",
        "\n",
        "  def call(self, x):\n",
        "    \"\"\"\n",
        "    Passes the input tensor `x` through a feedforward network consisting of two \n",
        "    dense layers with `dff` hidden units and a `relu` activation function. \n",
        "    A `dropout_rate` is applied after the first dense layer to prevent overfitting. \n",
        "    The output of the feedforward network is added to the original input `x` via the \n",
        "    `Add()` layer. Finally, the output is normalized using the `LayerNormalization()` layer.\n",
        "\n",
        "    Args:\n",
        "        x (tf.Tensor): Input tensor with shape `(batch_size, seq_len, d_model)`.\n",
        "\n",
        "    Returns:\n",
        "        tf.Tensor: Output tensor with shape `(batch_size, seq_len, d_model)`.\n",
        "    \"\"\"\n",
        "    x = self.add([x, self.seq(x)])\n",
        "    x = self.layer_norm(x) \n",
        "    return x\n"
      ],
      "metadata": {
        "id": "VxZjK_F0Zduo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test the layer, the output is the same shape as the input:"
      ],
      "metadata": {
        "id": "te7V6ngUzVzG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sample_ffn = FeedForward(512, 2048)\n",
        "\n",
        "print(en_emb.shape)\n",
        "print(sample_ffn(en_emb).shape)"
      ],
      "metadata": {
        "id": "twb82mU8ZdsM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.4.8. The encoder\n",
        "\n",
        "The encoder consists of a `PositionalEmbedding` layer at the input and a stack of `EncoderLayer` layers. Where each `EncoderLayer` contains a `GlobalSelfAttention` and `FeedForward` layer.\n",
        "\n",
        "Let's first write class for `EncoderLayer` and out togheter `GlobalSelfAttention` and `FeedForward`, then use stack of `EncoderLayer` and `PositionalEmbedding` to build `Encoder`."
      ],
      "metadata": {
        "id": "lOApxGK4zeIi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderLayer(tf.keras.layers.Layer):\n",
        "  \"\"\"\n",
        "  A single layer in the transformer encoder stack.\n",
        "\n",
        "  Args:\n",
        "    d_model (int): The dimensionality of the input and output sequences.\n",
        "    num_heads (int): The number of attention heads to be used in the self-attention sub-layer.\n",
        "    dff (int): The number of hidden units in the feedforward sub-layer.\n",
        "    dropout_rate (float): The dropout rate to be applied after the self-attention sub-layer.\n",
        "\n",
        "  Attributes:\n",
        "    self_attention (GlobalSelfAttention): A self-attention layer.\n",
        "    ffn (FeedForward): A feedforward neural network layer.\n",
        "  \"\"\"\n",
        "  def __init__(self,*, d_model, num_heads, dff, dropout_rate=0.1):\n",
        "    super().__init__()\n",
        "\n",
        "    self.self_attention = GlobalSelfAttention(\n",
        "        num_heads=num_heads,\n",
        "        key_dim=d_model,\n",
        "        dropout=dropout_rate)\n",
        "\n",
        "    self.ffn = FeedForward(d_model, dff)\n",
        "\n",
        "  def call(self, x):\n",
        "    \"\"\"\n",
        "    Applies the forward pass of the encoder layer.\n",
        "\n",
        "    Args:\n",
        "      x (tf.Tensor): The input sequence tensor.\n",
        "\n",
        "    Returns:\n",
        "      tf.Tensor: The output sequence tensor.\n",
        "    \"\"\"\n",
        "    x = self.self_attention(x)\n",
        "    x = self.ffn(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "Xk7L3k9qZdp6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `EncoderLayer` class represents a single layer in the transformer encoder stack. It consists of two sub-layers: a self-attention layer and a feedforward neural network layer.\n",
        "\n",
        "The `__init__` function initializes the `ncoderLayer` object by creating its sub-layers. The self_attention layer is an instance of the `GlobalSelfAttention` class, which performs self-attention over the input sequence. The `num_heads` and `key_dim` parameters determine the number of attention heads and the dimensionality of the keys and values in each head, respectively. The `dropout_rate` parameter specifies the dropout rate to be applied after the `self-attention` sub-layer. The ffn sub-layer is an instance of the FeedForward class, which consists of two dense layers with `ReLU` activation, followed by a dropout layer.\n",
        "\n",
        "The call function is called to apply the forward pass of the `EncoderLayer`. The input sequence `x` is passed through the `self_attention` sub-layer, followed by the `ffn` sub-layer, and the resulting output sequence is returned."
      ],
      "metadata": {
        "id": "VnspUqJm6had"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(tf.keras.layers.Layer):\n",
        "  \"\"\"\n",
        "  A custom Keras layer that implements the encoder of a transformer-based\n",
        "  neural network architecture for natural language processing tasks such\n",
        "  as language translation or text classification.\n",
        "\n",
        "  Args:\n",
        "    num_layers (int): The number of layers in the encoder.\n",
        "    d_model (int): The dimensionality of the output space.\n",
        "    num_heads (int): The number of attention heads in the multi-head\n",
        "      self-attention mechanism.\n",
        "    dff (int): The dimensionality of the fully connected feedforward\n",
        "      network.\n",
        "    vocab_size (int): The size of the vocabulary of the input language.\n",
        "    dropout_rate (float): The dropout rate to use for regularization.\n",
        "\n",
        "  Attributes:\n",
        "    d_model (int): The dimensionality of the output space.\n",
        "    num_layers (int): The number of layers in the encoder.\n",
        "    pos_embedding (PositionalEmbedding): The layer that learns the position\n",
        "      embeddings for each token in the input sequence.\n",
        "    enc_layers (list): A list of `EncoderLayer` instances, one for each\n",
        "      layer in the encoder architecture.\n",
        "    dropout (Dropout): The dropout layer for regularization.\n",
        "\n",
        "  Methods:\n",
        "    call(x): The forward pass of the encoder layer.\n",
        "\n",
        "  Returns:\n",
        "    The output tensor of the encoder layer, which has shape\n",
        "    `(batch_size, seq_len, d_model)`.\n",
        "  \"\"\"\n",
        "  def __init__(self, *, num_layers, d_model, num_heads,\n",
        "               dff, vocab_size, dropout_rate=0.1):\n",
        "    super().__init__()\n",
        "\n",
        "    self.d_model = d_model\n",
        "    self.num_layers = num_layers\n",
        "\n",
        "    self.pos_embedding = PositionalEmbedding(\n",
        "        vocab_size=vocab_size, d_model=d_model)\n",
        "\n",
        "    self.enc_layers = [\n",
        "        EncoderLayer(d_model=d_model,\n",
        "                     num_heads=num_heads,\n",
        "                     dff=dff,\n",
        "                     dropout_rate=dropout_rate)\n",
        "        for _ in range(num_layers)]\n",
        "    self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
        "\n",
        "  def call(self, x):\n",
        "    \"\"\"\n",
        "    Perform forward pass of the `Encoder` layer.\n",
        "    \n",
        "    Args:\n",
        "    x: tensor of shape (batch_size, sequence_length) representing the input token IDs sequence.\n",
        "\n",
        "    Returns:\n",
        "    A tensor of shape (batch_size, sequence_length, d_model) representing the output after applying \n",
        "    the self-attention and feed-forward layers to the input sequence.\n",
        "    \"\"\"\n",
        "    # `x` is token-IDs shape: (batch, seq_len)\n",
        "    x = self.pos_embedding(x)  # Shape `(batch_size, seq_len, d_model)`.\n",
        "    \n",
        "    # Add dropout.\n",
        "    x = self.dropout(x)\n",
        "\n",
        "    for i in range(self.num_layers):\n",
        "      x = self.enc_layers[i](x)\n",
        "\n",
        "    return x  # Shape `(batch_size, seq_len, d_model)`."
      ],
      "metadata": {
        "id": "Heclt9k8ZdnS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code defines the `Encoder` class that is used in the Transformer architecture for natural language processing tasks such as language translation and language modeling.\n",
        "\n",
        "The `Encoder` class is a subclass of the `tf.keras.layers.Layer` class, which is a base class for implementing new layers in Keras.\n",
        "\n",
        "The `__init__` method initializes the `Encoder` object by defining the model parameters such as d_model (the size of the output space), `num_heads` (the number of heads in the multi-head attention mechanism), `dff` (the dimension of the feedforward network), `vocab_size` (the size of the vocabulary of input tokens), and `dropout_rate` (the rate of dropout to be applied to the outputs of the layer).\n",
        "\n",
        "The `pos_embedding` attribute initializes a `PositionalEmbedding` layer that adds positional information to the input tokens to take into account their position in the sequence.\n",
        "\n",
        "The `enc_layers` attribute initializes a list of `EncoderLayer` objects, which each implement the `EncoderLayer` functionality. The number of layers in the encoder is determined by the `num_layers` parameter.\n",
        "\n",
        "The `dropout` attribute initializes a dropout layer to apply dropout to the output of the layer.\n",
        "\n",
        "The `call` method is called when the layer is called on input data. It applies the positional embedding to the input tokens and then applies the dropout layer. It then iteratively applies the `EncoderLayer` object to the output of the previous layer. The final output is returned as a tensor of shape `(batch_size, seq_len, d_model)`."
      ],
      "metadata": {
        "id": "gO-cpmY7683o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's test encoder:"
      ],
      "metadata": {
        "id": "4DglgTLfz8RO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate the encoder.\n",
        "sample_encoder = Encoder(num_layers=4,\n",
        "                         d_model=512,\n",
        "                         num_heads=8,\n",
        "                         dff=2048,\n",
        "                         vocab_size=8500)\n",
        "\n",
        "sample_encoder_output = sample_encoder(pt, training=False)\n",
        "\n",
        "# Print the shape.\n",
        "print(pt.shape)\n",
        "print(sample_encoder_output.shape)  # Shape `(batch_size, input_seq_len, d_model)`."
      ],
      "metadata": {
        "id": "35QuLpLYZdk0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.4.9. The decoder\n",
        "\n",
        "Similar to the `Encoder`, the `Decoder` consists of a `PositionalEmbedding`, and a stack of `DecoderLayer`. And the decoder's stack is slightly more complex, with each `DecoderLayer` containing a `CausalSelfAttention`, a `CrossAttention`, and a `FeedForward` layer. \n",
        "\n",
        "Let's first write `DecoderLayer` then write `Encoder`.\n",
        "\n",
        "Let's define `DecoderLayer` class which is a building block for a transformer-based decoder in a sequence-to-sequence model. The class inherits from `tf.keras.layers.Layer`.\n",
        "\n",
        "The class has an `__init__` method that initializes the layer's parameters and sub-layers. It takes the following arguments:\n",
        "\n",
        "- `d_model`: The number of expected features in the input and output.\n",
        "- `num_heads`: The number of parallel attention heads.\n",
        "- `dff`: The number of neurons in the feedforward sub-layer.\n",
        "- `dropout_rate`: The dropout rate to be applied.\n",
        "\n",
        "The `call` method defines how to use the layer in the forward pass. It takes two arguments: `x` and `context`. `x` is the input to the decoder layer, which is passed through causal self-attention, cross-attention, and feedforward sub-layers to produce the output `x`. `context` is the output from the encoder layer which is used as the attention context for the cross-attention mechanism.\n",
        "\n",
        "The class contains the following sub-layers:\n",
        "\n",
        "- `causal_self_attention`: The causal self-attention layer that attends to the input sequence in a causal manner, i.e., predicting future tokens based on the previous ones.\n",
        "- `cross_attention`: The cross-attention layer that attends to the encoder output context to align the decoder output with the input sequence.\n",
        "- `ffn`: A feedforward sub-layer that applies a non-linear transformation to the output of the attention sub-layers.\n",
        "\n",
        "The `call` method also caches the last attention scores computed by the `cross_attention` sub-layer, which can be used for visualization and debugging purposes."
      ],
      "metadata": {
        "id": "RQH5UBjU9XCu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderLayer(tf.keras.layers.Layer):\n",
        "  \"\"\"\n",
        "  A single layer of the decoder in a transformer-based architecture.\n",
        "\n",
        "  Args:\n",
        "    d_model (int): The number of expected features in the input.\n",
        "    num_heads (int): The number of attention heads.\n",
        "    dff (int): The dimensionality of the feedforward network.\n",
        "    dropout_rate (float): The dropout rate to be applied.\n",
        "\n",
        "  Attributes:\n",
        "    causal_self_attention: An instance of the `CausalSelfAttention` layer.\n",
        "    cross_attention: An instance of the `CrossAttention` layer.\n",
        "    ffn: An instance of the `FeedForward` layer.\n",
        "    last_attn_scores: A tensor containing the last attention scores.\n",
        "\n",
        "  \"\"\"\n",
        "  def __init__(self,\n",
        "               *,\n",
        "               d_model,\n",
        "               num_heads,\n",
        "               dff,\n",
        "               dropout_rate=0.1):\n",
        "    super(DecoderLayer, self).__init__()\n",
        "\n",
        "    self.causal_self_attention = CausalSelfAttention(\n",
        "        num_heads=num_heads,\n",
        "        key_dim=d_model,\n",
        "        dropout=dropout_rate)\n",
        "    \n",
        "    self.cross_attention = CrossAttention(\n",
        "        num_heads=num_heads,\n",
        "        key_dim=d_model,\n",
        "        dropout=dropout_rate)\n",
        "\n",
        "    self.ffn = FeedForward(d_model, dff)\n",
        "\n",
        "  def call(self, x, context):\n",
        "    \"\"\"\n",
        "    Forward pass of the `DecoderLayer`.\n",
        "\n",
        "    Args:\n",
        "      x (tf.Tensor): The input tensor of shape \n",
        "      `(batch_size, target_seq_len, d_model)`.\n",
        "      context (tf.Tensor): The context tensor of shape \n",
        "      `(batch_size, input_seq_len, d_model)`.\n",
        "\n",
        "    Returns:\n",
        "      The output tensor of the `DecoderLayer` of shape \n",
        "      `(batch_size, target_seq_len, d_model)`.\n",
        "\n",
        "    \"\"\"\n",
        "    x = self.causal_self_attention(x=x)\n",
        "    x = self.cross_attention(x=x, context=context)\n",
        "\n",
        "    # Cache the last attention scores for plotting later\n",
        "    self.last_attn_scores = self.cross_attention.last_attn_scores\n",
        "\n",
        "    x = self.ffn(x)  # Shape `(batch_size, seq_len, d_model)`.\n",
        "    return x"
      ],
      "metadata": {
        "id": "JW3EiXHFZdiD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's put this inside Decoder and write `decoder` class. This class is responsible for decoding the encoded input sequences to generate the target sequences in sequence-to-sequence models. The Decoder layer consists of multiple `DecoderLayer` blocks, with each block containing a self-attention mechanism, a cross-attention mechanism, and a feedforward network. The `Decoder` layer also includes positional embedding and dropout layers."
      ],
      "metadata": {
        "id": "8kc4DSHrKqJ4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(tf.keras.layers.Layer):\n",
        "  \"\"\"A decoder model for sequence to sequence learning.\n",
        "  \n",
        "  This class implements a decoder layer for a transformer-based model used for sequence to sequence learning tasks. The decoder layer takes input embeddings, positional encodings, and attention masks as input, and returns the output of the decoder layer after applying a multi-head self-attention mechanism, followed by a cross-attention mechanism with the output from the encoder layers, and then applying a feed-forward neural network.\n",
        "\n",
        "  Attributes:\n",
        "    d_model (int): The number of output dimensions for each layer.\n",
        "    num_layers (int): The number of layers in the decoder.\n",
        "    pos_embedding (PositionalEmbedding): The positional embedding layer.\n",
        "    dropout (Dropout): A dropout layer.\n",
        "    dec_layers (list): A list of DecoderLayer objects.\n",
        "    last_attn_scores (ndarray): The attention scores from the last decoder layer.\n",
        "    \n",
        "  Methods:\n",
        "    call(x, context): Implements the forward pass for the decoder layer.\n",
        "      Args:\n",
        "        x (ndarray): A tensor of shape (batch_size, target_seq_len), representing the input token IDs.\n",
        "        context (ndarray): A tensor of shape (batch_size, input_seq_len, d_model), representing the output from the encoder layers.\n",
        "      Returns:\n",
        "        ndarray: A tensor of shape (batch_size, target_seq_len, d_model), representing the output from the decoder layers.\n",
        "  \"\"\"\n",
        "  def __init__(self, *, num_layers, d_model, num_heads, dff, vocab_size,\n",
        "               dropout_rate=0.1):\n",
        "    super(Decoder, self).__init__()\n",
        "\n",
        "    self.d_model = d_model\n",
        "    self.num_layers = num_layers\n",
        "\n",
        "    self.pos_embedding = PositionalEmbedding(vocab_size=vocab_size,\n",
        "                                             d_model=d_model)\n",
        "    self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
        "    self.dec_layers = [\n",
        "        DecoderLayer(d_model=d_model, num_heads=num_heads,\n",
        "                     dff=dff, dropout_rate=dropout_rate)\n",
        "        for _ in range(num_layers)]\n",
        "\n",
        "    self.last_attn_scores = None\n",
        "\n",
        "  def call(self, x, context):\n",
        "    \"\"\"\n",
        "    Implements the forward pass for the decoder layer.\n",
        "\n",
        "    Args:\n",
        "      x (ndarray): A tensor of shape (batch_size, target_seq_len), representing the input token IDs.\n",
        "      context (ndarray): A tensor of shape (batch_size, input_seq_len, d_model), representing the output from the encoder layers.\n",
        "\n",
        "    Returns:\n",
        "      ndarray: A tensor of shape (batch_size, target_seq_len, d_model), representing the output from the decoder layers.\n",
        "    \"\"\"\n",
        "    # `x` is token-IDs shape (batch, target_seq_len)\n",
        "    x = self.pos_embedding(x)  # (batch_size, target_seq_len, d_model)\n",
        "\n",
        "    x = self.dropout(x)\n",
        "\n",
        "    for i in range(self.num_layers):\n",
        "      x  = self.dec_layers[i](x, context)\n",
        "\n",
        "    self.last_attn_scores = self.dec_layers[-1].last_attn_scores\n",
        "\n",
        "    # The shape of x is (batch_size, target_seq_len, d_model).\n",
        "    return x"
      ],
      "metadata": {
        "id": "iaFR3eKJ9Wp5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `Decoder` class expects as input a sequence of token-IDs representing the target sequence, and the encoded input sequence, or context, to which the decoder should attend to. The class consists of a stack of `DecoderLayer` instances, each of which applies a series of operations on the input sequence to generate an output sequence.\n",
        "\n",
        "In the constructor, the `Decoder` class initializes several layer instances, including a `PositionalEmbedding` layer, which adds positional encoding to the input token-IDs, a `dropout` layer, and a stack of `DecoderLayer` instances.\n",
        "\n",
        "During a forward pass, the input token-IDs are first passed through the positional embedding and dropout layers. Then, for each `DecoderLayer`, the input is passed through a causal self-attention layer, followed by a cross-attention layer, and finally through a feed-forward neural network layer. The output of the last `DecoderLayer` is returned as the output of the `Decoder`.\n",
        "\n",
        "The `last_attn_scores` attribute of the `Decoder` instance contains the attention scores from the last decoder layer, which can be useful for visualization and debuggin"
      ],
      "metadata": {
        "id": "XrwcSb47LVeP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate the decoder.\n",
        "sample_decoder = Decoder(num_layers=4,\n",
        "                         d_model=512,\n",
        "                         num_heads=8,\n",
        "                         dff=2048,\n",
        "                         vocab_size=8000)\n",
        "\n",
        "output = sample_decoder(\n",
        "    x=en,\n",
        "    context=pt_emb)\n",
        "\n",
        "# Print the shapes.\n",
        "print(en.shape)\n",
        "print(pt_emb.shape)\n",
        "print(output.shape)"
      ],
      "metadata": {
        "id": "xVWRCKhR9Wni"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_decoder.last_attn_scores.shape  # (batch, heads, target_seq, input_seq)"
      ],
      "metadata": {
        "id": "oDjPqtQS9WlO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.5. The Transformer\n",
        "\n",
        "The `Encoder` and `Decoder` are the key components of the Transformer model, but they need to be combined and followed by a final `Dense` layer to output token probabilities. Now let's put toghther these two classes and create the `Transformer` by extending `tf.keras.Model`:"
      ],
      "metadata": {
        "id": "Vpvn4VbtLsW8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer(tf.keras.Model):\n",
        "  \"\"\"\n",
        "  A transformer model that consists of an encoder, a decoder and a final dense layer.\n",
        "\n",
        "  Args:\n",
        "    num_layers (int): Number of layers in both the encoder and decoder.\n",
        "    d_model (int): Hidden size of the model.\n",
        "    num_heads (int): Number of attention heads used in the model.\n",
        "    dff (int): Size of the feedforward layer in the encoder and decoder.\n",
        "    input_vocab_size (int): Size of the vocabulary of the input.\n",
        "    target_vocab_size (int): Size of the vocabulary of the target.\n",
        "    dropout_rate (float): Dropout rate applied to the output of each sub-layer.\n",
        "\n",
        "  Attributes:\n",
        "    encoder (Encoder): An instance of the Encoder class.\n",
        "    decoder (Decoder): An instance of the Decoder class.\n",
        "    final_layer (Dense): A Dense layer that converts the final transformer output to output token probabilities.\n",
        "\n",
        "  Methods:\n",
        "    call(inputs): Forward pass of the transformer model.\n",
        "\n",
        "  Returns:\n",
        "    logits (tf.Tensor): Output tensor of the final dense layer. Shape (batch_size, target_len, target_vocab_size).\n",
        "  \"\"\"\n",
        "  def __init__(self, *, num_layers, d_model, num_heads, dff,\n",
        "               input_vocab_size, target_vocab_size, dropout_rate=0.1):\n",
        "    super().__init__()\n",
        "    self.encoder = Encoder(num_layers=num_layers, d_model=d_model,\n",
        "                           num_heads=num_heads, dff=dff,\n",
        "                           vocab_size=input_vocab_size,\n",
        "                           dropout_rate=dropout_rate)\n",
        "\n",
        "    self.decoder = Decoder(num_layers=num_layers, d_model=d_model,\n",
        "                           num_heads=num_heads, dff=dff,\n",
        "                           vocab_size=target_vocab_size,\n",
        "                           dropout_rate=dropout_rate)\n",
        "\n",
        "    self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n",
        "\n",
        "  def call(self, inputs):\n",
        "    \"\"\"\n",
        "    Forward pass of the transformer model.\n",
        "\n",
        "    Args:\n",
        "      inputs (tuple): A tuple of two tensors. The first tensor is the context input tensor of shape (batch_size, context_len).\n",
        "                      The second tensor is the target input tensor of shape (batch_size, target_len).\n",
        "\n",
        "    Returns:\n",
        "      logits (tf.Tensor): Output tensor of the final dense layer. Shape (batch_size, target_len, target_vocab_size).\n",
        "    \"\"\"\n",
        "\n",
        "    # To use a Keras model with `.fit` you must pass all your inputs in the\n",
        "    # first argument.\n",
        "    context, x  = inputs\n",
        "\n",
        "    context = self.encoder(context)  # (batch_size, context_len, d_model)\n",
        "\n",
        "    x = self.decoder(x, context)  # (batch_size, target_len, d_model)\n",
        "\n",
        "    # Final linear layer output.\n",
        "    logits = self.final_layer(x)  # (batch_size, target_len, target_vocab_size)\n",
        "\n",
        "    try:\n",
        "      # Drop the keras mask, so it doesn't scale the losses/metrics.\n",
        "      # b/250038731\n",
        "      del logits._keras_mask\n",
        "    except AttributeError:\n",
        "      pass\n",
        "\n",
        "    # Return the final output and the attention weights.\n",
        "    return logits"
      ],
      "metadata": {
        "id": "P-PMGP6o9WiZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `Transformer` class is a Keras model that combines the `Encoder` and `Decoder` to implement the Transformer architecture.\n",
        "\n",
        "The `Encoder` is an instance of the `Encoder` class that takes a sequence of tokens as input and outputs a sequence of vectors that represent the contextual information for each token in the sequence.\n",
        "\n",
        "The `Decoder` is an instance of the `Decoder` class that takes a sequence of target tokens and the contextual information from the `Encoder` as input and outputs a sequence of vectors that represent the contextual information for each target token in the sequence.\n",
        "\n",
        "The `final_layer` is a Keras Dense layer that takes the output of the `Decoder` and maps it to a sequence of target token probabilities.\n",
        "\n",
        "The `call` method of the `Transformer` class takes an input tensor `inputs`, which is a tuple of two tensors: the `context` tensor, which is the input sequence to the `Encoder`, and the x tensor, which is the target sequence to the `Decoder`. The method passes the `context` tensor through the `Encoder` to obtain the contextual information for each token in the sequence, and then passes the `x` tensor and the `Encoder` output to the `Decoder` to generate the output sequence. Finally, the method passes the output of the Decoder through the `final_layer` to obtain the target token probabilities. The method returns the `logits`, which are the target token probabilities, as well as the attention weights."
      ],
      "metadata": {
        "id": "1anuCVBdMw-G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In order to maintain a relatively quick and compact example, the size of the layers, embeddings, and internal dimensionality of the FeedForward layer in the Transformer model have been decreased. The original Transformer paper utilized a base model with `num_layers=6`, `d_model=512`, and `dff=2048`. However, the number of `self-attention` heads in this example remains the same, set at num_heads=8."
      ],
      "metadata": {
        "id": "qWC5rCM8NV52"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_layers = 4\n",
        "d_model = 128\n",
        "dff = 512\n",
        "num_heads = 8\n",
        "dropout_rate = 0.1"
      ],
      "metadata": {
        "id": "o65sM0Bs9Wfz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's instantiate the `Transformer` model:"
      ],
      "metadata": {
        "id": "t0Uc1UPQNnRz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transformer = Transformer(\n",
        "    num_layers=num_layers,\n",
        "    d_model=d_model,\n",
        "    num_heads=num_heads,\n",
        "    dff=dff,\n",
        "    input_vocab_size=tokenizers.pt.get_vocab_size().numpy(),\n",
        "    target_vocab_size=tokenizers.en.get_vocab_size().numpy(),\n",
        "    dropout_rate=dropout_rate)"
      ],
      "metadata": {
        "id": "kstC4GFC9WdO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can test the model before move forward to training part:"
      ],
      "metadata": {
        "id": "2ploBDHBNwZs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "output = transformer((pt, en))\n",
        "\n",
        "print(en.shape)\n",
        "print(pt.shape)\n",
        "print(output.shape)"
      ],
      "metadata": {
        "id": "GXl7UvrV9WZk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's print summary of model to visualize it better. "
      ],
      "metadata": {
        "id": "Cg6jLDFdN1dh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transformer.summary()"
      ],
      "metadata": {
        "id": "-i-lzk1F9WVy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The total number of trainable parameters in this model is 10,184,162. There are no non-trainable parameters."
      ],
      "metadata": {
        "id": "o-T55c5aOENy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.6. Training"
      ],
      "metadata": {
        "id": "XuoeG22kObyh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.6.1. Optimizer"
      ],
      "metadata": {
        "id": "hkj8CkuvUk4f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use the Adam optimizer with a custom learning rate scheduler according to the formula in the original Transformer [paper](https://arxiv.org/abs/1706.03762)."
      ],
      "metadata": {
        "id": "goFafjotWs0A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
        "  \"\"\"\n",
        "  Custom learning rate schedule that implements the learning rate function\n",
        "  described in the original Transformer paper. The learning rate is increased\n",
        "  linearly for the first `warmup_steps` training steps, and then decreased\n",
        "  proportionally to the inverse square root of the step number.\n",
        "\n",
        "  Args:\n",
        "    d_model (int): the dimensionality of the model.\n",
        "    warmup_steps (int): the number of steps taken to increase the learning rate\n",
        "      linearly. Default is 4000.\n",
        "  \n",
        "  Attributes:\n",
        "    d_model (float): the dimensionality of the model as a float.\n",
        "    warmup_steps (int): the number of steps taken to increase the learning rate\n",
        "      linearly.\n",
        "\n",
        "  Methods:\n",
        "    __call__(step): returns the learning rate at the given step.\n",
        "\n",
        "  Returns:\n",
        "    The learning rate at the given step.\n",
        "  \"\"\"\n",
        "  def __init__(self, d_model, warmup_steps=4000):\n",
        "    super().__init__()\n",
        "\n",
        "    self.d_model = d_model\n",
        "    self.d_model = tf.cast(self.d_model, tf.float32)\n",
        "\n",
        "    self.warmup_steps = warmup_steps\n",
        "\n",
        "  def __call__(self, step):\n",
        "    \"\"\"\n",
        "    Returns the learning rate at the given step.\n",
        "\n",
        "    Args:\n",
        "      step (int): the current training step.\n",
        "\n",
        "    Returns:\n",
        "      The learning rate at the given step as a float32 tensor.\n",
        "    \"\"\"\n",
        "    step = tf.cast(step, dtype=tf.float32)\n",
        "    arg1 = tf.math.rsqrt(step)\n",
        "    arg2 = step * (self.warmup_steps ** -1.5)\n",
        "\n",
        "    return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)"
      ],
      "metadata": {
        "id": "QUB7VqatUkne"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `CustomSchedule` class is a subclass of `tf.keras.optimizers.schedules.LearningRateSchedule`. It takes in two arguments `d_model` and `warmup_steps`. The `d_model` represents the dimensionality of the model, which is cast into a `float32`. The `warmup_steps` is the number of steps to increase the learning rate linearly before decaying it.\n",
        "\n",
        "The `__call__` method takes in a single argument `step`, which represents the current training step. It casts the step into a `float32` and calculates two arguments `arg1` and `arg2`. `arg1` is calculated by taking the `reciprocal` square root of the `step`. `arg2` is calculated by multiplying the step with the reciprocal square root of the `warmup_steps` raised to the power of `-1.5`.\n",
        "\n",
        "Finally, the method returns the product of the reciprocal square root of `d_model` and the minimum of `arg1` and `arg2`. This method is used to define the learning rate schedule for the optimizer used in the training process of the `Transformer` model.\n",
        "\n",
        "Now we can instantiate the optimizer (in this example it's `tf.keras.optimizers.Adam`):"
      ],
      "metadata": {
        "id": "GSWNWXv3WxA1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = CustomSchedule(d_model)\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98,\n",
        "                                     epsilon=1e-9)"
      ],
      "metadata": {
        "id": "DkRosAjCUkkd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's see how that looks like"
      ],
      "metadata": {
        "id": "wenzyx7gXdU8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(learning_rate(tf.range(40000, dtype=tf.float32)))\n",
        "plt.ylabel('Learning Rate')\n",
        "plt.xlabel('Train Step')"
      ],
      "metadata": {
        "id": "kkP_-PTMUket"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.6.1. Loss"
      ],
      "metadata": {
        "id": "CQ5G3HUhUrGm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's define loss and accuracy function. We are using cross-entropy loss function using `tf.keras.losses.SparseCategoricalCrossentropy`: "
      ],
      "metadata": {
        "id": "QKorl9P9XqUe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def masked_loss(label, pred):\n",
        "  \"\"\"\n",
        "  Calculates the masked sparse categorical cross-entropy loss between the true labels and predicted labels.\n",
        "\n",
        "  Args:\n",
        "      label: A tensor of shape (batch_size, seq_length) containing the true labels.\n",
        "      pred: A tensor of shape (batch_size, seq_length, target_vocab_size) containing the predicted labels.\n",
        "\n",
        "  Returns:\n",
        "      A scalar tensor representing the masked loss value.\n",
        "\n",
        "  \"\"\"\n",
        "  mask = label != 0\n",
        "  loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True, reduction='none')\n",
        "  loss = loss_object(label, pred)\n",
        "\n",
        "  mask = tf.cast(mask, dtype=loss.dtype)\n",
        "  loss *= mask\n",
        "\n",
        "  loss = tf.reduce_sum(loss)/tf.reduce_sum(mask)\n",
        "  return loss"
      ],
      "metadata": {
        "id": "7hNclceIUkZt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `masked_loss` function calculates the masked sparse categorical cross-entropy loss between the predicted values and the true labels. In this function, the `label` and `pred` inputs are the true labels and predicted values, respectively.\n",
        "\n",
        "First, the function creates a boolean mask to exclude padded values (0's) from the loss calculation. Then, it defines the loss object as `SparseCategoricalCrossentropy` from `tf.keras.losses`, which computes the cross-entropy loss between the true and predicted labels.\n",
        "\n",
        "The next step multiplies the loss by the mask to exclude any loss contribution from the padded values. The function then reduces the loss by computing the sum of the loss over the non-padded values and dividing it by the sum of the mask to obtain the average loss over non-padded values.\n",
        "\n",
        "Now let's write a function to calculate accuracy:"
      ],
      "metadata": {
        "id": "12Q6AFGDYNb1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def masked_accuracy(label, pred):\n",
        "  \"\"\"\n",
        "  Calculates the masked accuracy between the true labels and predicted labels.\n",
        "\n",
        "  Args:\n",
        "      label: A tensor of shape (batch_size, seq_length) containing the true labels.\n",
        "      pred: A tensor of shape (batch_size, seq_length, target_vocab_size) containing the predicted labels.\n",
        "\n",
        "  Returns:\n",
        "      A scalar tensor representing the masked accuracy value.\n",
        "\n",
        "  \"\"\"\n",
        "  pred = tf.argmax(pred, axis=2)\n",
        "  label = tf.cast(label, pred.dtype)\n",
        "  match = label == pred\n",
        "\n",
        "  mask = label != 0\n",
        "\n",
        "  match = match & mask\n",
        "\n",
        "  match = tf.cast(match, dtype=tf.float32)\n",
        "  mask = tf.cast(mask, dtype=tf.float32)\n",
        "  return tf.reduce_sum(match)/tf.reduce_sum(mask)"
      ],
      "metadata": {
        "id": "qYnnVpnYYLsq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `masked_accuracy` function computes the masked accuracy of the predicted values given the true labels. The inputs of the function are `label` and `pred`, which are the true labels and predicted values, respectively.\n",
        "\n",
        "First, the function uses `tf.argmax` to find the index of the maximum value in `pred` along the last dimension, which represents the predicted class. Then, the true labels label are cast to the same data type as the predicted values pred.\n",
        "\n",
        "The function then creates a boolean mask to exclude padded values from the calculation. The function uses the & operator to compare the predicted and true labels and create a boolean matrix match. The values in match indicate whether the predicted and true labels match for non-padded values.\n",
        "\n",
        "The match matrix and the mask matrix are then cast to `float32` and used to compute the average accuracy over non-padded values. The function returns the sum of match divided by the sum of mask."
      ],
      "metadata": {
        "id": "lKbAbe_mZjU4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.6.3. Training"
      ],
      "metadata": {
        "id": "bYixjv6BUsf9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's compile the model and use `model.fit` to train model!"
      ],
      "metadata": {
        "id": "BmiA5DEraY5N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transformer.compile(\n",
        "    loss=masked_loss,\n",
        "    optimizer=optimizer,\n",
        "    metrics=[masked_accuracy])"
      ],
      "metadata": {
        "id": "g8U7dVe09WTF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transformer.fit(train_batches,\n",
        "                epochs=20,\n",
        "                validation_data=val_batches)"
      ],
      "metadata": {
        "id": "E2XSvNE4OHKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.7. Testing"
      ],
      "metadata": {
        "id": "OMhHa6yfOegj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we trained our model for 20 Epochs, let's try to use this model for translating! For this, let's write `Translator` class. \n",
        "\n",
        "`Translator` takes `tokenizers` and `transformer` as inputs in its constructor. It has a `__call__` method that takes a sentence in Portuguese and translates it to English using the transformer model.\n",
        "\n",
        "The input sentence is first tokenized using the Portuguese tokenizer and converted to a tensor. The encoder input is set to be the tokenized sentence.\n",
        "\n",
        "The English `[START]` token is added to the output to initialize it. The output is stored in a `tf.TensorArray`.\n",
        "\n",
        "For each token in the output, the transformer model is called with the encoder input and the current output. The last token from the `seq_len` dimension of the predictions is selected and appended to the output. If the last token is the `[END]` token, the loop is terminated. The output is converted to text using the English tokenizer and returned along with the attention weights."
      ],
      "metadata": {
        "id": "hY16oj4ZaodB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Translator(tf.Module):\n",
        "  \"\"\"A translator that uses a transformer model to translate \n",
        "  sentences from Portuguese to English.\n",
        "\n",
        "  Attributes:\n",
        "      tokenizers (dict): A dictionary of tokenizers for the \n",
        "      Portuguese and English languages.\n",
        "      transformer (tf.keras.Model): A transformer model that can \n",
        "      be used for sequence-to-sequence translation.\n",
        "  \"\"\"\n",
        "  def __init__(self, tokenizers, transformer):\n",
        "    self.tokenizers = tokenizers\n",
        "    self.transformer = transformer\n",
        "\n",
        "  def __call__(self, sentence, max_length=MAX_TOKENS):\n",
        "    \"\"\"Translates a sentence from Portuguese to English.\n",
        "\n",
        "    Args:\n",
        "        sentence (str): The sentence to be translated.\n",
        "        max_length (int): The maximum number of tokens in the output sentence.\n",
        "\n",
        "    Returns:\n",
        "        tuple: A tuple containing the translated text, the tokens of the translated text,\n",
        "            and the attention weights of the transformer model.\n",
        "    \"\"\"\n",
        "    # The input sentence is Portuguese, hence adding the `[START]` and `[END]` tokens.\n",
        "    assert isinstance(sentence, tf.Tensor)\n",
        "    if len(sentence.shape) == 0:\n",
        "      sentence = sentence[tf.newaxis]\n",
        "\n",
        "    sentence = self.tokenizers.pt.tokenize(sentence).to_tensor()\n",
        "\n",
        "    encoder_input = sentence\n",
        "\n",
        "    # As the output language is English, initialize the output with the\n",
        "    # English `[START]` token.\n",
        "    start_end = self.tokenizers.en.tokenize([''])[0]\n",
        "    start = start_end[0][tf.newaxis]\n",
        "    end = start_end[1][tf.newaxis]\n",
        "\n",
        "    # `tf.TensorArray` is required here (instead of a Python list), so that the\n",
        "    # dynamic-loop can be traced by `tf.function`.\n",
        "    output_array = tf.TensorArray(dtype=tf.int64, size=0, dynamic_size=True)\n",
        "    output_array = output_array.write(0, start)\n",
        "\n",
        "    for i in tf.range(max_length):\n",
        "      output = tf.transpose(output_array.stack())\n",
        "      predictions = self.transformer([encoder_input, output], training=False)\n",
        "\n",
        "      # Select the last token from the `seq_len` dimension.\n",
        "      predictions = predictions[:, -1:, :]  # Shape `(batch_size, 1, vocab_size)`.\n",
        "\n",
        "      predicted_id = tf.argmax(predictions, axis=-1)\n",
        "\n",
        "      # Concatenate the `predicted_id` to the output which is given to the\n",
        "      # decoder as its input.\n",
        "      output_array = output_array.write(i+1, predicted_id[0])\n",
        "\n",
        "      if predicted_id == end:\n",
        "        break\n",
        "\n",
        "    output = tf.transpose(output_array.stack())\n",
        "    # The output shape is `(1, tokens)`.\n",
        "    text = tokenizers.en.detokenize(output)[0]  # Shape: `()`.\n",
        "\n",
        "    tokens = tokenizers.en.lookup(output)[0]\n",
        "\n",
        "    # `tf.function` prevents us from using the attention_weights that were\n",
        "    # calculated on the last iteration of the loop.\n",
        "    # So, recalculate them outside the loop.\n",
        "    self.transformer([encoder_input, output[:,:-1]], training=False)\n",
        "    attention_weights = self.transformer.decoder.last_attn_scores\n",
        "\n",
        "    return text, tokens, attention_weights"
      ],
      "metadata": {
        "id": "luwTBk7oOHHL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create an instance of this `Translator` class, and try it out a few times:"
      ],
      "metadata": {
        "id": "-g0uNyG5cHK9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "translator = Translator(tokenizers, transformer)"
      ],
      "metadata": {
        "id": "7UXXBM_POHE7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's wrrite a function to translate sentences for us:"
      ],
      "metadata": {
        "id": "9lBD0qkIciNQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def print_translation(sentence, tokens, ground_truth):\n",
        "  \"\"\"\n",
        "  The print_translation function takes in three arguments: sentence, tokens, \n",
        "  and ground_truth and prints out the input sentence, the predicted translation \n",
        "  and the ground truth translation.\n",
        "\n",
        "  Args:\n",
        "\n",
        "    sentence: A string that represents the input sentence.\n",
        "    tokens: A tensor of integers that represents the predicted translation.\n",
        "    ground_truth: A string that represents the ground truth translation.\n",
        "  \n",
        "  Returns:\n",
        "    This function doesn't return anything, it just prints out the input sentence,\n",
        "    predicted translation and ground truth translation in a specific format.\n",
        "  \"\"\"\n",
        "  print(f'{\"Input:\":15s}: {sentence}')\n",
        "  print(f'{\"Prediction\":15s}: {tokens.numpy().decode(\"utf-8\")}')\n",
        "  print(f'{\"Ground truth\":15s}: {ground_truth}')"
      ],
      "metadata": {
        "id": "2qneKTBecYXS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can also write a function to return attention wights. Let's write that!"
      ],
      "metadata": {
        "id": "S4FEN4w2iDws"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "nT5864oXjBY-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_attention_head(in_tokens, translated_tokens, attention):\n",
        "  \"\"\"\n",
        "  Plots the attention weights for a single head of the attention mechanism.\n",
        "\n",
        "  Args:\n",
        "  - in_tokens: a tensor of shape (in_seq_length,) containing the input tokens.\n",
        "  - translated_tokens: a tensor of shape (out_seq_length,) containing the translated tokens.\n",
        "  - attention: a tensor of shape (out_seq_length, in_seq_length) containing the attention weights.\n",
        "\n",
        "  Returns:\n",
        "  None.\n",
        "  \"\"\"\n",
        "  # The model didn't generate `<START>` in the output. Skip it.\n",
        "  translated_tokens = translated_tokens[1:]\n",
        "\n",
        "  ax = plt.gca()\n",
        "  ax.matshow(attention)\n",
        "  ax.set_xticks(range(len(in_tokens)))\n",
        "  ax.set_yticks(range(len(translated_tokens)))\n",
        "\n",
        "  labels = [label.decode('utf-8') for label in in_tokens.numpy()]\n",
        "  ax.set_xticklabels(\n",
        "      labels, rotation=90)\n",
        "\n",
        "  labels = [label.decode('utf-8') for label in translated_tokens.numpy()]\n",
        "  ax.set_yticklabels(labels)"
      ],
      "metadata": {
        "id": "a4c2Uc6zjBy1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " def plot_attention_weights(sentence, translated_tokens, attention_heads):\n",
        "  \"\"\"\n",
        "  Plots the attention weights for each head of the transformer model.\n",
        "\n",
        "  Args:\n",
        "      sentence (str): The input sentence in Portuguese.\n",
        "      translated_tokens (tf.Tensor): The translated tokens in English.\n",
        "      attention_heads (list): The attention heads of the transformer model.\n",
        "\n",
        "  Returns:\n",
        "      None\n",
        "  \"\"\"\n",
        "  in_tokens = tf.convert_to_tensor([sentence])\n",
        "  in_tokens = tokenizers.pt.tokenize(in_tokens).to_tensor()\n",
        "  in_tokens = tokenizers.pt.lookup(in_tokens)[0]\n",
        "\n",
        "  fig = plt.figure(figsize=(16, 8))\n",
        "\n",
        "  for h, head in enumerate(attention_heads):\n",
        "    ax = fig.add_subplot(2, 4, h+1)\n",
        "\n",
        "    plot_attention_head(in_tokens, translated_tokens, head)\n",
        "\n",
        "    ax.set_xlabel(f'Head {h+1}')\n",
        "\n",
        "  plt.tight_layout()\n",
        "  plt.show()\n"
      ],
      "metadata": {
        "id": "a2aEEaNOiDTv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "`plot_attention_weights` is a function that plots the attention weights of all the attention heads in a multi-head attention mechanism. It takes in the input sentence, the translated output tokens, and the attention weights and creates a figure where each subplot represents one attention head.\n",
        "\n",
        "`plot_attention_head` is a helper function used by `plot_attention_weights`. It takes in the input tokens, the output tokens, and the attention matrix of a single attention head and plots a heatmap where the x-axis represents the input tokens and the y-axis represents the output tokens. It is called once for each attention head to create the subplots in the `plot_attention_weights` figure."
      ],
      "metadata": {
        "id": "3evb6uSyi5Yy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "now let's put togheter some sentences to test:"
      ],
      "metadata": {
        "id": "m0JemIgwcNHL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = 'Eu amo programar em Python.'\n",
        "ground_truth = 'I love programming in Python.'\n",
        "\n",
        "translated_text, translated_tokens, attention_weights = translator(\n",
        "    tf.constant(sentence))\n",
        "print_translation(sentence, translated_text, ground_truth)\n",
        "\n",
        "plot_attention_weights(sentence, translated_tokens, attention_weights[0])"
      ],
      "metadata": {
        "id": "iHjzBJu7cMj6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = 'O tempo est timo para uma caminhada no parque.'\n",
        "ground_truth = 'The weather is great for a walk in the park.'\n",
        "\n",
        "translated_text, translated_tokens, attention_weights = translator(\n",
        "    tf.constant(sentence))\n",
        "print_translation(sentence, translated_text, ground_truth)\n",
        "\n",
        "plot_attention_weights(sentence, translated_tokens, attention_weights[0])\n",
        "\n",
        "plot_attention_weights(sentence, translated_tokens, attention_weights[0])"
      ],
      "metadata": {
        "id": "iGQ8OByTcMhx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = 'Eu preciso comprar po e leite na padaria.'\n",
        "ground_truth = \"I need to buy bread and milk at the bakery.\"\n",
        "\n",
        "translated_text, translated_tokens, attention_weights = translator(\n",
        "    tf.constant(sentence))\n",
        "print_translation(sentence, translated_text, ground_truth)\n",
        "\n",
        "plot_attention_weights(sentence, translated_tokens, attention_weights[0])"
      ],
      "metadata": {
        "id": "QCGETncycMfE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = 'A tecnologia tem transformado profundamente a forma como vivemos e trabalhamos.'\n",
        "ground_truth = \"Technology has profoundly transformed the way we live and work.\"\n",
        "\n",
        "translated_text, translated_tokens, attention_weights = translator(\n",
        "    tf.constant(sentence))\n",
        "print_translation(sentence, translated_text, ground_truth)\n",
        "\n",
        "plot_attention_weights(sentence, translated_tokens, attention_weights[0])"
      ],
      "metadata": {
        "id": "Yrvde46scMcp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = 'A pandemia de COVID-19 teve um impacto devastador na economia global.'\n",
        "ground_truth = \"The COVID-19 pandemic has had a devastating impact on the global economy.\"\n",
        "\n",
        "translated_text, translated_tokens, attention_weights = translator(\n",
        "    tf.constant(sentence))\n",
        "print_translation(sentence, translated_text, ground_truth)\n",
        "\n",
        "plot_attention_weights(sentence, translated_tokens, attention_weights[0])"
      ],
      "metadata": {
        "id": "u4qZ0zIDcMZs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.9. Experimenting with the Model\n",
        "\n",
        "Unfortunately, I don't have access to a lot of computing power, but it would be ideal to train the model for more epochs. I have tested the model with more than 20 epochs, so let's take a look at the results here for model with 50 epochs."
      ],
      "metadata": {
        "id": "CHTPtLZsj4Ob"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transformer.compile(\n",
        "    loss=masked_loss,\n",
        "    optimizer=optimizer,\n",
        "    metrics=[masked_accuracy])\n",
        "\n",
        "transformer.fit(train_batches,\n",
        "                epochs=50,\n",
        "                validation_data=val_batches)"
      ],
      "metadata": {
        "id": "eQgX-TAoknZY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "translator = Translator(tokenizers, transformer)"
      ],
      "metadata": {
        "id": "5VXO2TJpkz-i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = 'Eu amo programar em Python.'\n",
        "ground_truth = 'I love programming in Python.'\n",
        "\n",
        "translated_text, translated_tokens, attention_weights = translator(\n",
        "    tf.constant(sentence))\n",
        "print_translation(sentence, translated_text, ground_truth)\n",
        "\n",
        "plot_attention_weights(sentence, translated_tokens, attention_weights[0])"
      ],
      "metadata": {
        "id": "ojfVku3pkz1E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = 'O tempo est timo para uma caminhada no parque.'\n",
        "ground_truth = 'The weather is great for a walk in the park.'\n",
        "\n",
        "translated_text, translated_tokens, attention_weights = translator(\n",
        "    tf.constant(sentence))\n",
        "print_translation(sentence, translated_text, ground_truth)\n",
        "\n",
        "plot_attention_weights(sentence, translated_tokens, attention_weights[0])\n",
        "\n",
        "plot_attention_weights(sentence, translated_tokens, attention_weights[0])"
      ],
      "metadata": {
        "id": "t3Jhm7H4k1JQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = 'Eu preciso comprar po e leite na padaria.'\n",
        "ground_truth = \"I need to buy bread and milk at the bakery.\"\n",
        "\n",
        "translated_text, translated_tokens, attention_weights = translator(\n",
        "    tf.constant(sentence))\n",
        "print_translation(sentence, translated_text, ground_truth)\n",
        "\n",
        "plot_attention_weights(sentence, translated_tokens, attention_weights[0])"
      ],
      "metadata": {
        "id": "KjzNBJGmk1AS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = 'A tecnologia tem transformado profundamente a forma como vivemos e trabalhamos.'\n",
        "ground_truth = \"Technology has profoundly transformed the way we live and work.\"\n",
        "\n",
        "translated_text, translated_tokens, attention_weights = translator(\n",
        "    tf.constant(sentence))\n",
        "print_translation(sentence, translated_text, ground_truth)\n",
        "\n",
        "plot_attention_weights(sentence, translated_tokens, attention_weights[0])"
      ],
      "metadata": {
        "id": "WeliTB2xk05G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = 'A pandemia de COVID-19 teve um impacto devastador na economia global.'\n",
        "ground_truth = \"The COVID-19 pandemic has had a devastating impact on the global economy.\"\n",
        "\n",
        "translated_text, translated_tokens, attention_weights = translator(\n",
        "    tf.constant(sentence))\n",
        "print_translation(sentence, translated_text, ground_truth)\n",
        "\n",
        "plot_attention_weights(sentence, translated_tokens, attention_weights[0])"
      ],
      "metadata": {
        "id": "jVajwI-okzTZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "So it seems we have slightly better translation!"
      ],
      "metadata": {
        "id": "p0qZBLxfk8pp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.8. Export the model"
      ],
      "metadata": {
        "id": "mn2hxW6zOoMc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For this, let's create class called `ExportTranslator`, then wrap `translator` in the ExportTranslator:"
      ],
      "metadata": {
        "id": "ypFEtlEelU8M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ExportTranslator(tf.Module):\n",
        "  \"\"\"\n",
        "  A class for exporting a trained Translator model as a `tf.Module` for inference.\n",
        "\n",
        "  Args:\n",
        "  translator (Translator): A trained instance of the Translator class.\n",
        "\n",
        "  Methods:\n",
        "  __call__(self, sentence):\n",
        "      Translates the input sentence to the target language and returns the translation.\n",
        "  \"\"\"\n",
        "  def __init__(self, translator):\n",
        "    self.translator = translator\n",
        "\n",
        "  @tf.function(input_signature=[tf.TensorSpec(shape=[], dtype=tf.string)])\n",
        "  def __call__(self, sentence):\n",
        "    \"\"\"\n",
        "    Translates the input sentence to the target language and returns the translation.\n",
        "\n",
        "    Args:\n",
        "    sentence (tf.string): The input sentence to translate.\n",
        "\n",
        "    Returns:\n",
        "    A string tensor representing the translated sentence.\n",
        "\n",
        "    \"\"\"\n",
        "    (result,\n",
        "     tokens,\n",
        "     attention_weights) = self.translator(sentence, max_length=MAX_TOKENS)\n",
        "\n",
        "    return result"
      ],
      "metadata": {
        "id": "61nDEoxzOHDG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "`ExportTranslator` class takes a `translator` object as input and exports it as a TensorFlow module. It has a `__call__` method that takes a single string argument sentence and returns the translation result of the translator object for that input sentence.\n",
        "\n",
        "The `__call__` method is decorated with `tf.function` and `input_signature` that specify the data type and shape of the input tensor. The sentence tensor has an empty shape and string data type. The `__call__` method calls the translator object with the input sentence and max_length argument set to MAX_TOKENS, and returns the translation result as a tensor."
      ],
      "metadata": {
        "id": "fdnEpr5Hl1gQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "translator = ExportTranslator(translator)"
      ],
      "metadata": {
        "id": "5XZtgKHTlpTw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since the model is decoding the predictions using `tf.argmax` the predictions are deterministic. The original model and one reloaded from its `SavedModel` should give identical predictions:"
      ],
      "metadata": {
        "id": "wlUIx0i6lnU0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "translator('Eu amo programar em Python.').numpy()"
      ],
      "metadata": {
        "id": "xUzOW7mNOG_-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can use `.save` method to save model!"
      ],
      "metadata": {
        "id": "U3Xw5hkimJyY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tf.saved_model.save(translator, export_dir='translator')"
      ],
      "metadata": {
        "id": "d5Hv4sDbOG9i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can reload model and test the prediction!"
      ],
      "metadata": {
        "id": "3SbMYeLQmQNY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "reloaded_translator = tf.saved_model.load('translator')"
      ],
      "metadata": {
        "id": "_QwpgjYVOG67"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reloaded_translator('este  o primeiro livro que eu fiz.').numpy()"
      ],
      "metadata": {
        "id": "I4oSPvZqOG4Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **Final Note before goodbye:** This tutorial is heavily based on [https://www.tensorflow.org/text/tutorials/transformer](https://www.tensorflow.org/text/tutorials/transformer). I tried to make the explanation clear and adapt it based on my student taking this series of courses! I highly recommend going through the main material!"
      ],
      "metadata": {
        "id": "cuFSOoLomXiw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## References\n",
        "\n",
        "[1] Vaswani, Ashish, et al. \"Attention is all you need.\" Advances in neural information processing systems 30 (2017). \n",
        "\n",
        "[2] [https://python.plainenglish.io/image-captioning-with-an-end-to-end-transformer-network-8f39e1438cd4](https://python.plainenglish.io/image-captioning-with-an-end-to-end-transformer-network-8f39e1438cd4)\n",
        "\n",
        "[3] [https://www.tensorflow.org/text/tutorials/transformer](https://www.tensorflow.org/text/tutorials/transformer)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Gyj-LyhsslyB"
      }
    }
  ]
}